<!DOCTYPE html>
<!-- saved from url=(0095)https://canvas.asu.edu/courses/200987/pages/module-2-learning-materials?module_item_id=14590467 -->
<html dir="ltr" lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><script class="w-json-ld" type="application/ld+json" id="w-json-ldwistia_37">{"@context":"http://schema.org/","@id":"https://fast.wistia.net/embed/iframe/s9fr1h3qgi","@type":"VideoObject","duration":"PT11M5S","name":"13-Independent-Events","thumbnailUrl":"https://embed-ssl.wistia.com/deliveries/404ca69d7250f672f58e1684646bd4e1ecc3ad3d.jpg?image_crop_resized=640x405","embedUrl":"https://fast.wistia.net/embed/iframe/s9fr1h3qgi","uploadDate":"2021-09-10T22:46:13.000Z","description":"a DAT401 - Samara video","contentUrl":"https://embed-ssl.wistia.com/deliveries/04020492bf517983756abdb5d24eb232dcdeb402.m3u8","transcript":"Now we will talk about independent events. Let's look at the example, suppose we toss a coin Two times and let each one denote getting heads in the first toss and age two is event of getting heads in the second toss probability that we get heads in the second toss is one half. Now question what if we get heads in the first toss? Does that affect our level of certainty about getting heads in the second toss?\n\nWell, of course not. So conditional probability of getting heads in the second toss, given that in the first toss we got heads is still one half. In some sense, we can say that event H2 is independent of event H1. More generally, if and B are to events such that the probability of A given B is the same as probability of a.\n\nIt means that our knowledge that we have does not affect our level of certainty or belief about event a intuitively, again, we have that A does not depend on B and since probability of A given B is this fraction, which is defined when P of B is not zero. In order to talk about whether this identity is true or not, we would need to assume the probability of B is not zero. However, we will define independence in a bit more general sense in which we will not require probability of B to necessarily be different from 0. But still, when that is the case, it will be equivalent with this identity.\n\nThat probability of A given B is the same as probability of a. Now, if B of A given B is p of a, we already said in some sense that means that A is independent of b, but then what about P of B given A equals B of b? In other words, if we know that event A happened, does that affect our level of certainty about event b? Just like for this relation, we would say that A does not depend on by analogy or switching the roles of and B.\n\nThis would mean that B does not depend on a. So we wonder if A does not depend on b, then does that mean B does not depend on a.? Well, this is not so obvious immediately, but the answer is yes, as long as the probability of A is not equal to zero. So suppose that we do have that.\n\nA does not depend on the way we see it here. In other words, B of A given B is p of and apart from P of B not being equal to zero. Suppose also the p of a is not zero. And then biconditional probability, B given A probability of that is this fraction where in the numerator we have probability of the intersection.\n\nNow we use the other symmetric version of conditional probability by which we write probability of the intersection of and B as P of A given B times P of B. Now, if p of A given B is equal to p of a, then we can write this first term in the numerator as P of and after cancellation, we get p of B So assuming that this is true, we arrive at P of B given A equals P of B. In other words, this implies this. And, of course, by switching the roles of and b, we also have that this implies this.\n\nNow let us assume that one of these identities is true and thus so is the other one. Then we have p of a is P of A given B by that assumption, on one hand, and on the other hand, we write conditional probability of A given B as this ratio. And now multiplying by p of b, we get that p of AB is the product p of a times B of B. So assuming that both p of and P of B are not zero so that we can talk about conditional probability of A given B And conditional probability of B given a, we have that the following are equivalent that p of A given B is p of a that P of B given A is p of b, and that probability of the intersection of and B is the product of probabilities.\n\nNow, when we look at the third identity, we can talk about it regardless whether one of these events have probability 0 and we actually use this third identity as a definition of independent events. But this first and the second identity are useful to understand what exactly it means for two events to be independent. And now we have the definition of independence, our official definition, if and B are events of some sample space, as we say, that they are independent if probability of the intersection of and B is product of their probabilities. Again, recall that AB means intersection.\n\nWe all meet the symbol for intersection. Now let's see an example, choose a card at random from a regular deck of cards. And if we denote by age event that we choose hearts suit and by q event that we choose queen. Now the question is, are age and q events independent?\n\nWell, we have that probability of choosing hearts is 13 out of 52 because there are 13 denominations and/or 13 kinds, and each has won hearts suit. So 13 out of 52 cards. That's the probability of choosing hearts and the probability of choosing a queen. There are only four Queens, and probability of choosing a queen is 4 out of 52 and there is only one card that is queen of hearts.\n\nSo 1 out of 52 is probability of choosing queen of hearts, and we see that probability of choosing queen of hearts is the same as the product of probabilities of choosing hearts and probability of choosing queen. So we conclude that H and are independent events. Next, a couple of remarks. First, it is a common misconception that independence implies this jointness, actually, if and B are events whose probability is positive, then this is actually complete opposite of true.\n\nIn other words, if two events are independent, then they cannot be disjoint if we also assume that perhaps not zero. Namely, to see that. Let us suppose that and B are independent. In other words, that this is true.\n\nAnd then if their intersection was the empty set, then probability of their intersection. In other is, the empty set is 0 on one hand. On the other hand, by Independence is the product of these two probabilities, which means that one of the events A or B must have probability 0. So if and B are both of positive probability, then if they are independent, they cannot be disjoint.\n\nThe other remark in this course, we already use the word independence when we talked about calculus and functions of two or more variables. Well, that's independence in calculus sense, and that has nothing to do with the notion of independent events that we defined moments ago. This latter is stochastic independence or in stochastic or probabilistic sense. In both cases, one can reasonably justify why this term was used.\n\nBut you should be aware that the same word independence in these two situations has two completely different meanings, and we have the third remark. In practice, we rarely check independence computationally like we did in the example with hearts and queen events. Instead, we rely on description on the problem in the particular question and decide whether two or more events are treated independent or not. Just like we had at the beginning when we talked about getting heads in the first coin toss and getting heads in the second coin toss, we do not check that independence.\n\nWe only assume that our level of certainty about getting heads in the second toss is completely independent or will not be changed. Whether we know that in the first toss we got heads or we got tails.","potentialAction":{"@type":"SeekToAction","target":"https://canvas.asu.edu/courses/200987/pages/module-2-learning-materials?module_item_id=14590467&wtime={seek_to_second_number}","startOffset-input":"required name=seek_to_second_number"}}</script><script class="w-json-ld" type="application/ld+json" id="w-json-ldwistia_58">{"@context":"http://schema.org/","@id":"https://fast.wistia.net/embed/iframe/wfeuor0ci5","@type":"VideoObject","duration":"PT17M6S","name":"20-Continuous-RVs","thumbnailUrl":"https://embed-ssl.wistia.com/deliveries/9abc101b15355acedc928b0df36b9c3adcb84eb8.jpg?image_crop_resized=640x405","embedUrl":"https://fast.wistia.net/embed/iframe/wfeuor0ci5","uploadDate":"2021-09-10T22:46:39.000Z","description":"a DAT401 - Samara video","contentUrl":"https://embed-ssl.wistia.com/deliveries/4b1c5dc9c7a3043e1ee7d09efd1fe35ce20722ca.m3u8","transcript":"Continuous random variables, just like discrete, had already been discussed in that three Oh one, the prerequisite for this class, but not as thoroughly as we are going to see them and use them in this class. So we hear apart from a brief review. We are also going to discuss them in a bit more depth. We first start introducing a cumulative distribution function, and in fact, if you took that 300 which is not prerequisite for this class, then definitely this is going to be review for you.\n\nSo cumulative distribution function that corresponds to random variable capital X is a function defined in this way. So for any real number lowercase x, the value of CDF cumulative distribution function is probability that random variable capital X is less than or equal to little x. And then we define continuous random variable as a random variable who is a CDF. Cumulative distribution function is continuous at any point x now, as it often happens, continuous random variable may have a probability density, and that's a function little f So that for any real number x, the value of CDF is equal to the integral from minus infinity to x of this density function.\n\nAnd in that case? So if you see this density, the graphical interpretation of this relation between CDF and density is, as you can see, the value capital F of x is the area below the density curve over the interval from minus infinity up to x. And in that case, we also have that probability that capital X falls into any interval between two values and B can be computed as the integral from a to B of f of XDX. Also, if you recall, when we discussed choosing a point randomly from an interval from 0 to two, we said that probability of choosing any single point is equal to zero.\n\nAnd so that's actually in general property for any continuous random variable. And that actually means graphically that whether you include the end point A in this interval. So whether you have open interval from A to B or semi-open, including a, but not including B is irrelevant, the probability is the same because probability that x takes value exactly equal to a is equal to zero graphically. That just means that the area below the density curve over the interval from a to B is the same regardless.\n\nWhether you include in this area, in this region, regardless, whether you include this vertical line x equals a or not. And similarly is with the including or not including the vertical line x equals B. So you can either include both of them or just one of them, and the area will be the same, so that actually means that these all probabilities are equal to the probability that x is in between and B. This, of course, is in contrast with discrete random variable, where you do need to worry and do need to be cautious, whether you include an end point of a certain interval or not, the probability would change if the endpoint of the interval is in the range of the random variable.\n\nBut again, for continuous random variables, we do not need to worry about that. And any of these four probabilities is equal to the difference. The CDF at B minus CDF at a. And that's easy to see from here, from the definition of a capital F and the density.\n\nIf of any X capital F of any x is this cumulative area over the interval from minus infinity up to that value, so f of B is the area below the density from minus infinity up to be. So that means we included this part. And then when you subtract f of a, that means you exactly exclude this part below the density curve and the left from the point a. And finally, one to note, any continuous random variable in practice does have density, but in probability theory, one can construct random variables that have continuous CDF and therefore by definition are continuous random variables, but to do not have density.\n\nHowever, such constructs are more of an interest of theory in practice. Any continuous random variable that you come up with does have density, so you do not need to worry about that. And in probability theory, continuous random variable that does have density is called absolutely continuous, although we will equate the two notions as it is usually done in undergraduate statistics courses. So when we say continuous random variable, we will actually mean absolutely continuous random variable.\n\nIn other words, we will mean random variable that has density. Finally, by definition of CDF and its relation with the density, we see that capital F is a derivative of the density. In other words, the value of the density little f at point x is exactly the derivative of capital F at that point. So that's actually fundamental theorem of calculus, and this is another version.\n\nI think it's usually called the second fundamental theorem of calculus, which is immediate consequence of the first one that this integral is equal to this difference. So this relation is actually what you have discussed in calculus. Our first example of continuous random variable would be the distribution that we discussed in the previous videos. So standard normal, which was obtained by starting from binomial, then we transformed it to get the essence star, which wasn't binomial.\n\nAnd as n increases, essence star is getting more and more possible values and two consecutive values of essence star are getting closer and closer to each other, so more and more dense grid of points on x-axis. And when we looked at a rescaled probability histogram so reskilled, that was when probability that essence star takes certain value is equal to the area of the corresponding bar corresponding rectangle in the histogram. In that way, the area of the entire probability histogram rescaled probability histogram is one. And in that case, as n increases, the shape of the histogram, the silhouette converges to some curve.\n\nAnd that happens to be this curve. And we call the random variable z that can take any real number as a possible value. We call that random variable z standard normal. And this example also justified the notion of density.\n\nSo that this definition here does not look artificial. And the probability that z falls in between and b, so that this shaded region here, and that's approximately also probability that essence star is in between and b, and the approximation is getting better and better as n increases. And we also had more generally the entire family of a random variables, normal random variable family with the parameters mu and sigma squared. And as we play with you, any real number that is a constant and sigma, which is also constant but must be positive.\n\nWe have that for any such choice of mu and sigma. We get a new density. And this is true in general in our general definition of continuous random variable with density little f. Once you specify lowercase f the density function of the random variable, you completely determine the distribution of that random variable in the sense that you can find probability that x belongs to any interval.\n\nAnd from there, you can combine intervals taking unions, intersections and so on, so you can also determine probability that x belongs to more complicated sets. And lastly, let's see some examples that involves computations in art. So this is density of standard, normal of distribution, so mean 0 standard deviation or sigma equal to 1. And let's take values and B being, as you can see here.\n\nSo a is here. Negative 0.3 and B is here. 1.4 What is the probability that this standard normal, which I denoted by capital x, falls in between these two values and b, in other words, that we see observed value being somewhere in between this number and this number.\n\nGraphically, that's this area. And we use pnorm that's a function in R ps for probability, cumulative probability, norm for normal at point B In other words, this is CDF of standard normal evaluated a B minus pnorm at a. So we are applying exactly this. And so the value, as you can see, is five, three, 7 and so on.\n\nAnd here is a nice way to review some bits and pieces of r. So this code plot this graph, as you can see here. Pa function is for setting parameters, and I'm controlling here margins and outer margins. So in the figure, there is an inner part that is controlled by Ma and outer part control by ohm a.\n\nYou can see documentation for R And then I am creating a sequence of x values from minus 4 to 4 by 0.01. And then I plot on x-axis is x en x is the norm. Now this is new, maybe, or you have discussed, possibly in that 301. So this is for density of normal.\n\nSo you CP is for cumulative probability pnorm for cumulative probability of normal. D is for density, the norm for density normal. If you want binomial, this would be p Bynum and D Bynum would be the value at the given point. So probability mass function in the case of binomial.\n\nSo the norm at all these values x is a vector. Mean is 0 as D is one, so you can control the parameters mu and sigma and these are actually default values. So this is standard. Normal type equals L.\n\nThat means draw a line and a B line horizontal equals zero. That's just the drawing. This horizontal line y equals 0 because by default, you won't see it. That takes care of this curve and this x-axis.\n\nAnd to draw polygon a line here, I extract only those values of x that are in between and B. Recall that here inside the brackets I have here and logical operation. And this gives me all indices of x that satisfy both these two conditions that a is less than or equal to x And that x is less than or equal to B. So I extract only those values x that are in between these two values.\n\nThen I create the corresponding y values. So that's this part, and then I create vertices of this region and to plot this region, I use polygon. You can see documentation by typing question mark and polygon. Hexadecimal collar, eight C1 D 40 is for maroon, one of the ASU colors.\n\nDensity equals 15 deaths. How rare these shaded lines are and angle equals 45 is actually default angle of these shaded lines. So that's how you get this plot, and you can play around with this code and modify to see what each part does.","potentialAction":{"@type":"SeekToAction","target":"https://canvas.asu.edu/courses/200987/pages/module-2-learning-materials?module_item_id=14590467&wtime={seek_to_second_number}","startOffset-input":"required name=seek_to_second_number"}}</script><script class="w-json-ld" type="application/ld+json" id="w-json-ldwistia_46">{"@context":"http://schema.org/","@id":"https://fast.wistia.net/embed/iframe/kwrq5519mf","@type":"VideoObject","duration":"PT7M6S","name":"16-PMF-Prob-Histogram","thumbnailUrl":"https://embed-ssl.wistia.com/deliveries/7dbac3753f91eddf28ca6313b4b8712ab6048bf5.jpg?image_crop_resized=640x405","embedUrl":"https://fast.wistia.net/embed/iframe/kwrq5519mf","uploadDate":"2021-09-10T22:46:24.000Z","description":"a DAT401 - Samara video","contentUrl":"https://embed-ssl.wistia.com/deliveries/8224177e0d846dcc1f6799d895900d8bdcf4bb36.m3u8","transcript":"In this video, we discuss how we can describe discrete random variable analytically and how it can be visualized. Recall our example of random variable x being number of heads in three coin tosses. So for each outcome, we assign the number of heads for that outcome. And once we established which outcome from x results in which number, we can also produce the following table table about probabilities.\n\nWell, x is one of the possible values that random variable capital X can take. So zero, one, two or 3 and we see probability that x is 0 is 1 eight. That's because x takes value 0 if and only if the outcome DTT happened. And since that's just one out of eight outcomes, and we assume all outcomes are equally likely because we assume the coin is fair.\n\nSo we have one out of eight the probability of getting 0 probability of getting value 1. Well, one happens. So one had in the three coin tosses. Well, that happens if we have h t, THT or TTH.\n\nSo three outcomes give rise to the value 1. So 3 out of 8. So that's the probability of getting value 1. And similarly, for getting value to well, it's symmetric.\n\nThat's also 3/8. And finally, probability of x taking value 3 is 1 out of eight. And note that the sum of all these four values is equal to 1. And of course, that's to be expected because probability of getting one of these four numbers is one, because whenever we run the experiment, we always get one of the four values.\n\nSo this table completely determines for what value in the range of capital X we have, what probability for that value to happen. We say this table describes the distribution of random variable x. And we can also visualize this graphically. So this is so-called probability histogram so that for each of these four values, 0 through three, we make a rectangle over that value whose height is equal to the corresponding probability.\n\nSo for the first rectangle, that corresponds to x equals zero, we have the height one here. The height is three, eight, three, 8 and 1 eighth, although we can see that the distribution is symmetric even based on the table. It's easier to see this symmetry by looking at the probability histogram. Also in that three Oh one, you've seen histograms from the data, from the collected data, but this histogram probability histogram is a theoretical distribution of this random variable x.\n\nAlso, it is common that we widen the bars so that two consecutive bars share one side. And later, we will mostly see such histograms. Now back to this table, from this table, we can derive a function, a mapping x little x Any possible value of random variable x to the probability that capital X takes that little x And that is called probability mass function. So in general, if x is a discrete random variable, with the range being this set here, then mapping from x Any x in the range to probability that capital X takes that value x, that function is called probability mass function.\n\nAnd as we already mentioned, this table and thus probability mass function completely determines the distribution of x. In other words, if you know, probability mass function of random variable x, you can determine probability that x belongs to any subset of the range, R x, or we can determine probability for any interval of real numbers. What's the probability that x belongs to that interval? We use abbreviation PMF for probability mass function, and whenever it is clear what the random variable x we are talking about, we can omit the subscript capital X So we can just write little p of little x.\n\nSo that's the probability that x is equal to x. Sometimes we use letter k, especially for discrete random variables. Also, to determine the distribution of random variable x, that actually means to determine probability mass function, in other words, to find X of little x for every x in its range. And example of probability mass function well, our example of number of heads in 3 coin tosses.\n\nSo based on this table, we see that the p of x is one eighth. If x is in the set of numbers 0 or 3 and be is 3/8 if x is 1 or two.","potentialAction":{"@type":"SeekToAction","target":"https://canvas.asu.edu/courses/200987/pages/module-2-learning-materials?module_item_id=14590467&wtime={seek_to_second_number}","startOffset-input":"required name=seek_to_second_number"}}</script><script class="w-json-ld" type="application/ld+json" id="w-json-ldwistia_70">{"@context":"http://schema.org/","@id":"https://fast.wistia.net/embed/iframe/s3kk8ib5gv","@type":"VideoObject","duration":"PT17M13S","name":"24-Population-Mean-and-Sample-Mean","thumbnailUrl":"https://embed-ssl.wistia.com/deliveries/6b3978f8f722427be071ac79e696be405ac735cd.jpg?image_crop_resized=640x405","embedUrl":"https://fast.wistia.net/embed/iframe/s3kk8ib5gv","uploadDate":"2021-09-10T22:49:25.000Z","description":"a DAT401 - Samara video","contentUrl":"https://embed-ssl.wistia.com/deliveries/f1b416a98a737129779ef02ecb4b1147d966cd42.m3u8","transcript":"In that three or one, we've discussed the mean and variance both population and the sample mean and variance in this class, we are going to review that and build upon that. And if you took that 300 math tools for data science, then this is going to be a review. OK, so here we have a normal distribution with two parameters mu and sigma squared sigma squared is positive. And here is the density.\n\nWe already had that and we mentioned that mu is the position of the peak of the distribution and the density is symmetric about mu. Recall our app for normal distribution. We said that as we move mu to the right or to the left, or increase or decrease, the effect of that to the population or to the density is that the density shifts. And that's also easy to see based on the definition of density.\n\nSo algebraically. In this way, a muse serves as a measure of central tendency in the sense that if we collect the sample from this distribution with the current value Musical The negative five, we will see most of the observations concentrated around the negative 5. Whereas if we see for mu equals eight, if we take a sample from this distribution with this mu equals 5 or six, then we will see our observations to be concentrated more around that number. Number 6 and in our other app for normal distribution, we had some sampling.\n\nHere we collect the sample of size 50. And by that, I mean that we start from normal population with the mu here equal to zero and the sigma, the parameter sigma equal to 1. And the way we collect these 50 observations is you can think of, let's say, seventh observation as the seventh random variable. With this normal distribution for these values, IMU and sigma and in general AI observation for AI between 1 and 50, we assume it to be normally distributed.\n\nAnd these n observations, when thought of as observations from any random variables, we assume that they are independent random variables all being distributed as normal populations and with these values, mu and sigma. This is so-called simple random sample of size n here of size 50. And similarly for the other two. And let me just extend the range of x.\n\nAnd so it's the same sample. I just rescaled the x-axis. And here we show the density of the original population, and we indeed see that the data are concentrated mostly around mu. Mew equals zero.\n\nAnd the same thing is with the other three. If I change IMU and shift a little bit and take another sample as we can see the sample moves. Actually, it's a new sample from a new distribution. It's still normal with the same sigma, but different mu.\n\nAny time I move u, the app samples from this new distribution and we see how the samples move as I change value of mu. This, of course, is as expected because probability that x takes value around you in some interval of, let's say, some fixed length around is equal to the area below the density curve over that interval. And since the density is highest here, we can expect much higher probability that we see observations in that interval than in some interval of the same length, but far away from you, because the density is, as we can see, almost flat zero very close to zero. We know it's not exactly zero, it's always positive, but it's very, very small.\n\nIf you move far away from mew, we can say that you measures central tendency of the population. Let's take another example binomial random variable. Here is a probability histogram of binomial random variable with n equal 10. So number of trials and the probability of success is 0.1.\n\nSo in each trial probability of success is 10% And we count how many successes in 10 trials. So this is probability histogram. And because p is fairly small, we expect higher probabilities for smaller total number of successes in 10 trials. For high number of trials, probability of success is fairly low or very, very low, barely positive, for example, for probability that x takes value 10.\n\nSo all 10 trials to be successful, probability for that should be 1/10 raised to the power of 10 or probability of success raised to the power of 10. So 10 to the negative 10 very small number. And when we take a sample, here is a r code, and this is the sample histogram or histogram that comes from the sample here I take n equals 50 a size of a sample. But the input argument size in this function r function r Bynum is actually the number of trials, so size equals 10.\n\nThat means 10 trials. So each of these 50 experiments consists of taking 10 trials and counting how many trials were successful in that particular experiment. And repeat that 50 times, and the r is in general for taking a random sample, followed by the name of the distribution or abbreviation. So our Bynum is taking a random sample from binomial.\n\nOur norm is for normal and so on. And then we plot the histogram. And this is what we see, which is pretty much in accordance with the theoretical probability histogram. Now what happens when I take a larger probability of success?\n\nSo p equals 0.5 and repeat sampling, so take a sample from this distribution, so it's still binomial. Still, n equals 10, but now probability of success in each trial being 0.5. And as we can see, this sample is more concentrated around five.\n\nWe also see it's a somewhat different shape or actually considerably different shape than in the case of sampling from this distribution. And that's not surprising, because when probability of success is 50% or one half in 10 trials, we in some sense expect to have about half of the time. So that means about five times to have successes. And when we look at the theoretical or probability histogram of this distribution with the p equals five, we see that it is symmetric about point five, about number 5 and that is in accordance with this histogram.\n\nThis sample, we see that it's of course, it's not ideally symmetric because the numbers taken are random. But if we were to take a much larger sample, we could expect the histogram to be more symmetric. Nevertheless, it's still decently symmetric and decently in accordance with the theoretical probability, histogram, theoretical population. And now we get to the point to define measure of central tendency.\n\nOne possible measure of central tendency and that's going to be expected value. And the way we define expected value is a point where you would put fulcrum so that probability histogram is in balance. In this case, due to symmetry, it should be five. Whereas when we have P equals one, it should be a number.\n\nSo that if we put fulcrum there, the probability histogram is in balance. Now, in this particular case, if we have 10 trials with probability of success, 1/10 in each trial, we expect about once to see success and this geometric or static approach as point of balance will give us indeed, the expected value in this case to be one, as we will see in the next video, expected value of binomial is n times p in general, and that is in accordance with what we just said, that we expect only once to see success in 10 trials if probability of success is 1/10. And so how do we define expected value?\n\nWell, for discrete case, it's weighted average, just like we said here, in order to put fulcrum so that the system is in balance point of gravity of the probability histogram should be weighted average of all the locations exi, all the possible values and weights are equal to the probabilities of the corresponding possible values. We denote the expected value in this way for random variable capital X also right music sub x And we call it either expected value or mean. This is a population expected value or population mean. We also have a sample mean, which we will discuss shortly.\n\nBut before that, how do we define expected value or mean for continuous random variable? Well, it's just going to be continuous version of this formula. And you may have had that in Calc 2 derivation of point of gravity of a region below some function. So in our case, the function in question is density, so it's not difficult to see.\n\nBut we are not going to derive that point of gravity below density of a continuous random variable is given in this way. So, for example, for this density to compute expected value of corresponding random variable capital X. So you need to put the fulcrum somewhere here. So that the whole region below the density is in balance.\n\nAnd how exactly you find the point of gravity or expected value? Well you use this formula. Recall that when we discussed normal and transitioning from binomial to normal, we scaled histogram to get probability histogram in which instead of probabilities being height, we had probabilities being areas of small rectangles. And here it's actually f of x times.\n\nSo that's the height times dx. So the increment. So this is, of course, in the limiting case. So the probability in discrete case, which is approximately area of rectangle, has converted to f of x times dx in the continuous or limiting case.\n\nAnd finally, when we do not know which distribution our sample is coming from, we would like to estimate it by computing sample mean the most reasonable way and the most intuitive way to compute an estimate for the true theoretical population. Mean is when we take a sample, so Xi is either observation, so x size our observations or x1 through xn and observations from population, and we just take average of them. Now the big difference between population mean and the sample mean is that sample mean is a random variable because from sample to sample, the observations vary and thus this average.\n\nSo x bar varies. So it's a random variable, whereas expected value of x is a constant. So one should not get confused. It's important to understand the difference between population mean and sample mean.","potentialAction":{"@type":"SeekToAction","target":"https://canvas.asu.edu/courses/200987/pages/module-2-learning-materials?module_item_id=14590467&wtime={seek_to_second_number}","startOffset-input":"required name=seek_to_second_number"}}</script><script class="w-json-ld" type="application/ld+json" id="w-json-ldwistia_64">{"@context":"http://schema.org/","@id":"https://fast.wistia.net/embed/iframe/xbvpah4u8n","@type":"VideoObject","duration":"PT16M49S","name":"22-KDE","thumbnailUrl":"https://embed-ssl.wistia.com/deliveries/47028c09722fe860bf3f90cc10f7fd43e5c59868.jpg?image_crop_resized=640x405","embedUrl":"https://fast.wistia.net/embed/iframe/xbvpah4u8n","uploadDate":"2021-09-10T22:49:18.000Z","description":"a DAT401 - Samara video","contentUrl":"https://embed-ssl.wistia.com/deliveries/135ad1d6ea4496dcf06f97bd4b8587bcc13a742a.m3u8","transcript":"And we now discuss kernel density estimate we saw in the previous video data set. Old faithful and eruptions, and we plotted the histogram of eruptions data and those data were collected from some continuous random variable. And while the histogram represents a discrete visualization of data and also of the underlying distribution underlying random variable, we would now like to estimate the density of that continuous random variable. And in fact, in R that is done using one way to do it is using function density, which gives us this curve, which is an estimate of the underlying theoretical density based on these collected data.\n\nIf we collect the different data, different values of eruptions, the density would look different in this call of function density applied to the vector of eruptions data. We use the parameters BW that's for bandwidth 0.15 and the kernel Gaussian, which means standard normal distribution. And let's see now how exactly we get this density.\n\nWell, instead of Gaussian or standard normal, there are other possible choices of kernel, which we denote by k and choosing some constant positive constant h that our bandwidth parameter. Here is the formula for kernel density estimate or kernel density function at any point x. In the case of applying Gaussian kernel in this formula, that means that we use function capital K defined in this way, which is the density of z distribution, the standard normal distribution. Now to explain where this formula comes from, why such formula?\n\nConsider this part of the histogram based on the histogram, it looks like there is a dramatic change in the distribution in this small region here, since this bar is twice as high or even more than the adjacent bar. And the difference is even bigger when we look at the bar on the left. However, when you look at the rug, it doesn't look like such a dramatic change happening so quickly. Moreover, if we move the brakes, so instead of brakes being at this point and this point and this point, if we shift everything to the right, just a little bit and let's say the first of the bars being from this point up to here and then the next one from this point up to here, of course, we would have to include the bar for those on the left.\n\nWell, these two adjacent bars would probably not be as dramatically different as we see them in this histogram. The moral of the story is that the choice of where you put the brakes does affect, even if you keep the number of breaks and the length of each brake or each bin, even you keep them the same if you just shift a little bit. The histogram may look differently. One way to avoid that is to consider at any point on the real line x to consider the counts, how many observations in the neighborhood of that point.\n\nIn other words, for any point on x axis, we take fixed bandwidth the size of a single bin. And instead of drawing a bar on the top of that value, we just draw a point where the height will be proportional to the counts or how many values in that neighborhood in that bandwidth. And as we move value x on x axis, we actually move the value y the corresponding height and that so-called moving histogram, which actually corresponds to this function when k is a uniform distribution. I will not talk about details and how we get this formula.\n\nI will only mention that capital K in that case is a uniform distribution on interval negative 1 one and that such kernel density estimate, this CD function actually is a step function. But let's see if we really want smooth function. How can we get a smooth kernel density estimate? Well, we can choose a smooth kernel density function to begin with to get smooth kde.\n\nAnd so let us consider Gaussian kernel and to do the analysis. I am going to plot the same histogram as the one that you see here. It's just around here. It's just that, Uh, I will use lower opacity so that the stronger color is not on our way.\n\nThat's one thing. And another thing is why limit goes up to two point five, as you can see here, rather than what is this point seven? The reason for that is I will use Gaussian kernel that go as high, so nothing else has changed. So it's the same histogram.\n\nIt's just that. Now, it looks smaller, but the proportions remain the same. So this bar here is twice as high or even a bit more than this one here, as it used to be the same ratio. Now what we want to do is take a point, so I'm choosing this one.\n\nAnd would like to determine the value of the kde, the estimating density at that point. So I would like to find this value the y-coordinate of this point. And here is what I do in moving histogram. If this is the point where I need to find the value of kde, I take bandwidth around this point.\n\nSo with the center at that point and just count how many observations I see in that bandwidth. And so that means I treat all the points that are in that interval that been equally regardless whether they are closer to my red point or not or are a bit further away. But if I choose Gaussian distribution, this normal density, I can treat points that are closer to the point of interest. The red point, I can treat those points closer to be more important than those that are further away.\n\nMoreover, unlike in moving histogram, I do not care whether points that are close to the beam of interest, but outside it are really close to the beam or not. Whereas in Gaussian, I actually count all the points I take into account all the points in the entire sample. But again, those that are closer to the red point. The point of interest are going to be treated as more important.\n\nAnd how do I value the importance? Well, exactly as the value of density function at that given observation. So for example, this point here is treated or is weighted by the value, which is the function normal density, this normal density evaluated at that point. So it's going to be some height this much.\n\nWhereas this point, let's say this point here is further away from my red point. And so it's treated as less important, and that level of importance is evaluated by the value of the red function, the normal density at that point. So it's going to be some value of this height. So the value of kde at my red point, in other words, this height here is equal to the average of heights of this red normal distribution evaluated at all x values that are in my sample all the observations from the sample.\n\nSo average of all those heights. If I take another point, this blue one here to estimate the value of kde at that blue point, I take the average of heights of this blue distribution blue density, which is just the same density as this red one, but shifted so that it's centered at the blue point rather than at red point. And the reason why the value of kde at the blue point is lower than at the red point is because simply there are less points in the vicinity of the blue, one for which the value of the blue density is fairly high. There are much more points that are further away from the blue one.\n\nAnd so for those values, for those observations, the corresponding value of the blue density is very low. So you see it's almost 0 for all these points here. Now, what is exactly the mathematical expression of this blue density end of this red density? Well, it's a transformed standard normal.\n\nFirst of all, the difference between blue and red is just a shift by the distance between blue and the red point. And if I want to make more influence of points that are in the very narrow vicinity of each of the point at which I estimate and make those that are further away much less influential, I would shrink and take the normal density with a smaller standard deviation or smaller variance. In other words, I would take normal with smaller sigma. Well, this sigma is actually what we call here bandwidth for this Gaussian kernel, and that's our h.\n\nSo in that case, I would transform standard normal by dividing the argument by h And so. This formula here with the original initial kernel density being standard normal, this age here dividing argument by h. I get a smaller standard deviation or larger depending whether h is smaller or larger. And for each observation x I, I have here x minus x PSI the difference or actually signed the distance between x and Xi and the further Xi is from my value x at which I estimate kde, the smaller the value of a standard normal density at the corresponding point x minus Xi over h.\n\nAnd one thing to mention, this blue density and this red density were not obtained starting from standard normal just by shifting the center or shifting the mean and the dividing argument by age. In fact, when I divide the argument, if I make this bandwidth smaller in order for me to keep it actually to be a density from some normal distribution, I also need to multiply by 1 over h. So that's a normalizing or scaling factor. It's not difficult to see that you do need to multiply by 1 over e.\n\nAnd in fact, here is the derivation that shows that, in other words, this computation here shows that such an estimated such a kde function indeed has a property that the integral is equal to 1. If you didn't multiply by 1 over h, this calculation would show the integral being h, so you need to adjust your f by dividing by h. I will not go through these details, but you can take a look at this computation.","potentialAction":{"@type":"SeekToAction","target":"https://canvas.asu.edu/courses/200987/pages/module-2-learning-materials?module_item_id=14590467&wtime={seek_to_second_number}","startOffset-input":"required name=seek_to_second_number"}}</script><script class="w-json-ld" type="application/ld+json" id="w-json-ldwistia_40">{"@context":"http://schema.org/","@id":"https://fast.wistia.net/embed/iframe/6nwiabgta4","@type":"VideoObject","duration":"PT14M54S","name":"14-Independence-of-Multiple-Events","thumbnailUrl":"https://embed-ssl.wistia.com/deliveries/af5a4c6669521400becf82b6b41533b54903c011.jpg?image_crop_resized=640x405","embedUrl":"https://fast.wistia.net/embed/iframe/6nwiabgta4","uploadDate":"2021-09-10T22:46:18.000Z","description":"a DAT401 - Samara video","contentUrl":"https://embed-ssl.wistia.com/deliveries/323befa068fb71e2b27a4d0599fd046b28e2bdb5.m3u8","transcript":"We now discuss independence of several events. Suppose A1 through an are some events from a sample space S. We say that this collection of events A1 through a n is a collection of independent events. If for any subcollection of, let's say, key events or for any k between 1 and mn, if we take any collection of key elements from this set of elements, A1 through a n, we have that probability of intersection of all the events from that subcollection is product of corresponding probabilities of events of that collection.\n\nAnd in particular for three events a one, a two, a three. Or we can denote them as a B and C This definition means that all these identities are satisfied. In other words, we can take a subcollection of three elements that's actually the entire collection of the events a, B and c, and we have this identity at the bottom, but also any subcollection of two elements. So and B and collection and C And collection B and C so if all these identities are satisfied, then we say that events a, B and C are independent.\n\nThis is maybe not what one would guess that independence of three events would be defined with. So usually one would guess that independence would be defined only by requiring the last of the identities that the probability of the intersection of all the three events is product of the probabilities. However, in practice, we often need to deal with not just the entire collection of the events that we want to call independent, but also with some subset or subcollection. And that's why we require this stronger condition.\n\nNote that this means that our definition of independence is stronger than pairwise independence. So and B are independent if just this first identity is true. And C are independent if just this second identity is true. And similarly for the third identity and independence of B and C.\n\nSo in general, if we just have pairwise independence of any pair of two events from our collection, A1 through a n That is a weaker than what we here require as independence of an events A1 through a n. So not only that, we need the pairwise independence, but any collection of three events and four events and so on. Also, as already mentioned, our definition is stronger than just the fourth identity on this list. Now let's see an example.\n\nIrregular tetrahedron is cast, so just like we cast the die, we now cast regular tetrahedron. That means for congruent faces or being equilateral, three faces are colored in one of the colors red, green and blue, and the fourth phase is colored with all the three colors. In other words, one phase of the tetrahedron is blue. Another one is red.\n\nThe one behind is green and the fourth phase, which in this picture is the one at the bottom. So this is fourth phase, so it's actually supposed to be equilateral. Well, that one is colored with all the three colors. And denote by r event that there is a red color on the down face after we cast the tetrahedron.\n\nSo unlike with a die, when we look at the top face and the number on it after the dye stops rolling. Well, in this case, since there is no top face, of course, we can only talk about down face after the tetrahedron stops rolling and r is the event that there is a red color on the down face. This actually means either this side on the right happened to be down or this side here happened to be down after the tetrahedron stops rolling. And similarly, G is event that there is a green color on the down face and similarly B for blue.\n\nNow question are the events are energy independent? Another question are rg and be independent? Well, without any computation, our intuition cannot say much about independence of our energy, nor our G and B. We could argue, however, that by symmetry, the pair of Rongji shares the same destiny as R&B, as well as G and B.\n\nBut let's see. We have that probability of getting red color on the down face is two out of four because we have two faces with red color out of four faces and similarly p of g and p of so that's one half now probability that we get both r and g. So both red and green color. Well, that only happens if we have the face that has all three colors.\n\nSo this is the same as event of the intersection of rg and B And that is just one phase out of four. So probability of that is one quarter. And similarly, p of R&B and p of GB. They are all one quarter.\n\nBut because all of these three probabilities p of RPG and P of B being one half, we can see that all of these intersections of any pair is the product of probabilities of corresponding events. This means that Rongji are independent. And so are R&B and so r, G and B. In other words, we can say that events are G and B are pairwise independent.\n\nHowever, the p of RGB is one quarter, but that's not the same as the product of probabilities of rg and B because the product is one eighth. So our G and B are not independent, although they are pairwise independent couple of remarks. First of all, the above example shows that indeed independence, the way we define it is strictly stronger than pairwise independence when we talk about three or more events. Also, sometimes as we in fact, already mentioned for two events, but the same is when we have multiple events, it is not intuitively clear whether some events are independent.\n\nIn other words, whether knowledge that one of them happened would affect our level of certainty about the other one. Such the case was in previous example when we had r in g, for example. However, in practice, we rarely check independence computationally like we did in this example and in the previous video when we talked about getting queen of hearts. So instead, in practice, we rely on description of the problem in question to decide whether two or more events are treated independent or not.\n\nLet's see an example. A coin is tossed three times the sample space in that case is this one with eight possible outcomes where, for example, means that in the first toss we got heads in the second we got tails in the third we got heads, and it is reasonable to assume that whether we get heads or tails in one or two tosses. This does not affect the outcome of the other tosses. In other words, the outcomes in the tosses are independent.\n\nSo if we denote by h.I, the ethos shows heads and the tie event that the toss shows tails, then probability of HHT by our assumption of independence of outcomes in different tosses. So writing this PhD is the intersection of h one, H2 and T3 by Independence. We have the disease product of the corresponding probabilities, and that's exactly one eighth.\n\nSo in this example, we assumed that whatever happens in different tosses will not affect what will happen in other tosses. In other words, that we have independence of these ages and tI's four different indices. I similarly, we can show for any of elementary outcomes that its probability is 1 eight, just like we showed for this particular one. Now again, this means the sample spaces with equally likely outcomes because all of these have probability one eighth and vice versa.\n\nIf we only assume that probability of HI is one half for every index, I want two or 3 and that all these outcomes are equally likely, it is easy to show that the outcomes in different tosses are independent. So, for example, that h one, H2 and T3 are independent events. Another example, suppose we cast the die twice and the note by a event of getting no. One in the first cast and B is event that we get no.\n\n6 in the second cast. Naturally, and B are independent probability of getting number six in. The second cast is one sixth. Regardless, whether we know that event A happened or didn't happen.\n\nAlso, probability of not getting number 6 is 5 six. Again, regardless whether we know that event A happened or didn't happen. In other words, probability of B complement given A is the same as probability of B complement, which is 5 six. So that means that when and B are independent, we also have that and B complement are independent, and that's actually true in general, not just in this case.\n\nSo we have a proposition if and B are independent events, so is the pair of events a compliment and B as well as and B complement like we saw here, as well as a compliment and B compliment? And the proof is easy. So suppose that and B are independent, we can write B as this composition B intersected with a union B intersected with a complement and then probability of the left side is some of the probabilities on the right because the union is disjoint. And now from there we can write p of a compliment B.\n\nSo this term here as P of B minus p of a b, so just move P of a the left. But now by Independence p of AB is p of eight times p of b, and after factoring out P of B and writing 1 minus p of as P of A compliment, we exactly see that a compliment and B are independent by our definition of independence. And now to show the other two parts that and B complement are also independent. Well, just switch the roles of and B because if and B are independent, so r, B and and argue in exactly this way to conclude that B complement and are independent and similarly now apply this result further to events and B complement to conclude that a compliment and B complement are also independent.\n\nAnd so that completes the proof.","potentialAction":{"@type":"SeekToAction","target":"https://canvas.asu.edu/courses/200987/pages/module-2-learning-materials?module_item_id=14590467&wtime={seek_to_second_number}","startOffset-input":"required name=seek_to_second_number"}}</script><script class="w-json-ld" type="application/ld+json" id="w-json-ldwistia_61">{"@context":"http://schema.org/","@id":"https://fast.wistia.net/embed/iframe/sc24zos85v","@type":"VideoObject","duration":"PT9M43S","name":"21-Histograms","thumbnailUrl":"https://embed-ssl.wistia.com/deliveries/5da9740917e85010f7ce846c63cc12fc3c6041e3.jpg?image_crop_resized=640x405","embedUrl":"https://fast.wistia.net/embed/iframe/sc24zos85v","uploadDate":"2021-09-10T22:48:57.000Z","description":"a DAT401 - Samara video","contentUrl":"https://embed-ssl.wistia.com/deliveries/66c9e98f1932c880c3944792b0d089918de9f1e6.m3u8","transcript":"In this video, we talk about histograms, a notion that you have seen not just in that three or one, but also probably much earlier in your life and at the beginning of our discussion, this would be more or less of a review from that 3, 2, 1. But then later, we will build upon that a new material. First of all, so far we have seen so-called probability histograms in particular probability histograms of binomial. And this ascent star probability histograms are visualization of true theoretical discrete distribution.\n\nSo that each height over a certain value on x-axis represents probability that random variable takes that value. And now we will talk about histograms that come from samples and the whole idea of histogram is to visualize data and see where they are more concentrated, where they are less concentrated to give us some idea about the distribution of the underlying random variable from which data were collected. And when I say histogram, I would primarily mean histogram that comes from a sample. So not theoretical probability distribution, so not probability histogram over discrete random variable.\n\nFor those, I will use the term probability histogram. So here is an example, a well-known data set faithful from our package data sets, which is already built in r, which comes with the installation. So you do not need to load it and head of faithful prints out first. Six observations and we see two variables eruptions and waiting by applying function STR for structure of faithful data.\n\nWe see it's a data frame, two variables both numerical. We have 272 observations. And of course, we see that there are two variables to columns. These data are collected from Old Faithful geyser at Nordstrom National Park in Wyoming, and eruption variable is duration of eruption of the geyser, followed by waiting time for the next eruption.\n\nAnd these data are given in minutes and let us visualize data eruptions variable. So it's numerical and it makes sense to plot the histogram so we declare or define variable erupts. It's variable from faithful data frame. Recall that we call variable from given data frame using the name of the data frame, followed by dollar sign, followed by the name of the variable the name of the column.\n\nAnd then we use the function. Hist. brakes equals 20 when this is just one numerical value. It divides real lines into this many beans or sub intervals, and the color equals red.\n\nNow, slight modification of this is when I add this function rugg, which plots here this rug like bottom part of the image, and each line represents one data, so one value of the vector erupts at the position that corresponds to the value. So, for example, a line here corresponds to the value. There is about 2.0. And we see there are data concentrated here and also concentrated here and in between.\n\nMuch less. And these bars, as you know, represent some kind of summary visual summary of the data. So this bar here is over the interval, as you can see here, and its height is proportional to the number of data. Number of values of a vector erupts that are in that interval.\n\nAnd that's true for every bar. And the higher the bar, the more data in the corresponding interval. But the reason why we see more values in this interval than, say in this interval is probably because it was much more likely to see values here than here. So when we gathered the data, more of them happened to be in this interval than in this interval, which means that this histogram gives us an idea about probability distribution of the underlying random variable.\n\nNow, the underlying random variable from which these eruptions data were collected. This is, of course, just a portion of them. In total, there are 272. This underlying random variable represents the time duration of the time.\n\nAnd of course, we can model this random variable to be a random variable that can take any value from 0 and hypothetically any positive number, although probably highly unlikely, very large number. So the underlying random variable can be thought of as a continuous random variable because it takes values from an interval, namely interval of all positive numbers interval from 0 to infinity. And we can talk about the density of that random variable. And this histogram gives us some idea about what the density might look like, at least roughly speaking, because wherever we see higher bars, we collected more data, which means it was more likely to see values from that region if the sample was correctly collected, so that it really represents the true distribution.\n\nAnd wherever it's more likely to see observations, that's where the underlying theoretical density is higher. Now it is a common need in data science to estimate density based on the data. We do not have any clue what the underlying density is, but when we collect the data, we can estimate the density and the way it is done is using kernel density estimate. That's a curve that you see here.\n\nAnd before I briefly describe Cady, I just want to mention the difference between this histogram. And this one is that here the heights are exactly the frequency, how many observations appeared here, whereas here we have a frequency equals false, in which case the heights are scaled so that the total area of the histogram is one so that this looks like probability distribution, so that this is a rescaled version of the previous histogram. And they do look the same, which is because we kept the proportions. Each height in this histogram was a scaled by the same quantity to get corresponding height in this histogram.\n\nAnd to keep each video relatively short, we discuss kernel density estimate in the following video.","potentialAction":{"@type":"SeekToAction","target":"https://canvas.asu.edu/courses/200987/pages/module-2-learning-materials?module_item_id=14590467&wtime={seek_to_second_number}","startOffset-input":"required name=seek_to_second_number"}}</script><script class="w-json-ld" type="application/ld+json" id="w-json-ldwistia_52">{"@context":"http://schema.org/","@id":"https://fast.wistia.net/embed/iframe/dqpmduckbv","@type":"VideoObject","duration":"PT10M39S","name":"18-Normal-RV-Part1","thumbnailUrl":"https://embed-ssl.wistia.com/deliveries/7a7ac42c9a0b918d7922a1bc03d17b76a9203b3f.jpg?image_crop_resized=640x405","embedUrl":"https://fast.wistia.net/embed/iframe/dqpmduckbv","uploadDate":"2021-09-10T22:46:31.000Z","description":"a DAT401 - Samara video","contentUrl":"https://embed-ssl.wistia.com/deliveries/5ed7c093ea64e45a81dbdbff1aaf336e10f15872.m3u8","transcript":"Normal random variable is one of the most commonly used random variables that appear in many modeling and also the most commonly used random variable when modeling errors in various phenomena. We shall now see how we can get normal if we start from binomial random variable. Well, let s.n. denotes binomial random variable, and we can think of it as number of successes in n independent trials with probability of success in each trial as p. Now we will be interested in what is happening when the number of trials and increases, when n goes to infinity.\n\nIn that H3O one, we discussed the expected value of a random variable as a measure of central tendency or point of gravity, where you would put fulcrum so that the corresponding histogram is in balance. And we also discussed the variance as a measure of spread and also standard deviation is another measure of spread of random variable. We will talk about these quantities more thoroughly in this class in the upcoming videos, but at this point, I just want to mention that binomial random variable Cn has average or expected value and times p, which is quite reasonable. For example, if you toss a coin 100 times, you expect about 50 times to get heads because probability of getting heads in a single trial is one half.\n\nAnd you have 100 trials, so 100 times one half. Also, variants happens to be an PCU, and standard deviation is square root of variance, so square root of n pq now when n, the number of trials increases and pq increases as well. So for a fixed probability of success in a single trial, p and p goes to infinity as n goes to infinity. This means when you draw probability histogram of s.n. as you increase n regardless what the fixed probability p is.\n\nWhen you increase mn, the bulk of the histogram shifts towards infinity. Also, when P and are fixed and you increase RN, the standard deviation square root of NPQ blows up as well goes to infinity. So not only that the probability histogram shifts is moving away towards infinity, but also at the same time it spreads out more and more. Now, in order to keep it centered or not allowing it to move away as we increase n and also not allowing it to completely spreads out.\n\nWe are going to rescale it or to normalize it or to standardize it by subtracting NPD, the expected value from the random variable s.n. and divide by the standard deviation. We will later discuss this transformation in more detail. But let essence star denotes this transformed random variable ascent, so essence star is not binomial anymore. It is still discrete random variable.\n\nAnd since s.n. takes values from 0 to RN, all the integers from 0 to n essence star also takes only n plus 1 values. But since four fixed RN, two consecutive values of s.n. differ by one, so we get the increments by one essence star, which is this quantity has two consecutive values differing by 1 over square root of n PCU because n is fixed and the ASN is taking values 0 1 all the way up to n. This means as we increase RN, two consecutive values of essence star are getting closer and closer to each other because the difference is 1 over square root of NPQ and as n goes to infinity, that quantity 1 over square root of mpic goes to zero.\n\nNow let's play around with this app when we increase n starting from 5 and let's keep p equals 0.5. So that's like in coin tossing. Probability of success is one half. And as we increase and what you see on the right is the histogram of essence star.\n\nAnd I'm increasing and and what happens? So probability, histogram, the way we defined it has heights that correspond to or are equal to the corresponding probabilities. But since we are spreading the probabilities to the larger and the larger possible number of values, namely from 0 to n all possible values n plus 1 many of them. If you increase RN, there are more and more possible values of essence star, and therefore it's not difficult to see that each possible value.\n\nIs taking smaller and smaller probability now, such probability histogram is not useful because we cannot see much what's going on in such distribution. So what we want to do now is instead of considering probability histogram to consider a rescaled version of it. Let me put n equals 5 back to 5 in this rescaled version or normalized version, the heights over the corresponding values, possible values of essence star are not equal to the corresponding probabilities. Instead, we want to rescale the heights so that instead of the height representing probability, we want the area of the corresponding bar to represent probability.\n\nIn this way, instead of the sum of the heights being equal to one, we will have the sum of the areas of rectangles being equal to one, and that will preserve a reasonable picture. So you see the height is always equal to 1 as I move n This slider along all possible values for n. So now the histogram does not flatten. Now back to n equals.\n\nFive so now the question is, what should this height be so that the area is equal to the corresponding probability? Well, the area of a rectangle is height times width. So we want the height to be the probability corresponding probability divided by the width. So this with this incremental delta x is 1 over square root of n q, so that when we divide the probability by delta x, that means the original probability should be multiplied by square root of n pq to get this rescaled histogram.\n\nAnd now let me keep increasing. As you can see this rescaled or normalized probability histogram. Is now approaching to some shape that looks closer and closer to some smooth curve. And what curve is that?\n\nWell, it turns out that this curve, this black curve that you see here and it's a value for every x is, as you can see here. This curve is so-called density of this curve is so-called density of the limiting random variable, the limiting distribution and the limiting distribution in question is standard normal random variable, often denoted by z. The significance of the density of random variables z is that it helps us to determine probability that z falls into some interval, which we will discuss in the next video.","potentialAction":{"@type":"SeekToAction","target":"https://canvas.asu.edu/courses/200987/pages/module-2-learning-materials?module_item_id=14590467&wtime={seek_to_second_number}","startOffset-input":"required name=seek_to_second_number"}}</script><script class="w-json-ld" type="application/ld+json" id="w-json-ldwistia_67">{"@context":"http://schema.org/","@id":"https://fast.wistia.net/embed/iframe/6ak31neeya","@type":"VideoObject","duration":"PT11M25S","name":"23-KDE-Example","thumbnailUrl":"https://embed-ssl.wistia.com/deliveries/5db2d2c807434a99be279ad993c172e53fe20d9c.jpg?image_crop_resized=640x405","embedUrl":"https://fast.wistia.net/embed/iframe/6ak31neeya","uploadDate":"2021-09-10T22:49:21.000Z","description":"a DAT401 - Samara video","contentUrl":"https://embed-ssl.wistia.com/deliveries/ffe7a959103e22dd3e2e895f90c3f4a05cba0494.m3u8","transcript":"Let's see an example of kernel density estimation here, we take a sample from normal distribution of size 100, and the way you can do that in R is by calling the function r norm. So R is for random. We saw the norm is for value of density of normal. P norm is value of cumulative probability and our norm is for randomized sample and sample sizes 100.\n\nAnd I'm taking here mean mu to be zero and the sigma standard deviation to be three. Previously, I set the seed to one two three four. So some specific numbers. So that you can replicate exactly the same histogram even though values are random.\n\nIn fact, they are, as it is said, pseudo random because you can set the seed and always get the same values of random sample. And so we plot the histogram. Freck equals falso frequency. It's not the frequency histogram, it's normalized histogram so that the area of the histogram is one of rugby colors red, green and blue.\n\nSo it's a purely red but alpha channel or opacity is 0.5. That's why you see the shade of red as shown here, and we plot. Apart from histogram, we also plot the rug and then we create using density function in base R. We create our CD and we use a band with two and the kernel Gaussian, by the way, the density function returns a list and there is a list of x coordinates, list of y-coordinates and you can explore other stuff other components of the list, such as bandwidth and sample size n, but we know from this code what they are.\n\nAnd apart from plotting k.d, we also plot the actual theoretical underlying density from which the distribution comes from. So this normal density? Now keep in mind that in kde, the algorithm or, Uh, this function here.\n\nDoes not know what the family of distributions the underlying theoretical distribution comes from, so it's not that we know that it's a normal and it's just that we need to estimate parameters. Instead, we don't know anything about distribution. The algorithm does not take into account any assumptions about the type of distribution, and that's so-called non-parametric estimate and a part of statistics that deals with such estimates are called non-parametric statistics. And now in our studio, we have the same code and I'm going to play around.\n\nSo what happens, for example, if I take another sample? So this one is better simply because the sample resembles the actual distribution better than the previous one. But what if I take another sample? So you see, this one is also not as good.\n\nThe reason for such variability is because sigma is three relatively large. But if I don't know what the distribution, underlying distribution the data come from. So if I only see the histogram of the data, if I want my CD to more closely follow the data, I would take a band with the smaller. But you see, it's more wiggly, and that's because it's more sensitive to the slight change in the data.\n\nNamely, you see here there is a high concentration of data and also here, that's why you see these two little bumps. Let's take this sample, and now you see it follows the data more closely the CD does than when I had the band with equal to 2. So to follow the data more closely, you make a bandwidth smaller. That actually means that the influence or the effect of points that are very close to the actual point at which we estimate CD is much higher and the only one relevant.\n\nSo those that are even a little bit further away from the point at which we are estimating CD are pretty much irrelevant. That's because smaller bandwidth or in our formula age, the smaller age is the smaller variability of this density curve. But the problem with taking small bandwidth or small age is that if you go from sample to sample, the obtained CD varies a lot from one sample to another, which means it's more likely that your estimate is quite away from the true density. This phenomenon is called the bias variance tradeoff, which is basically the trade between getting your estimate that more closely correspond to the data.\n\nIn other words, less bias you pay the price for less bias by having larger variability from sample to sample, and, in other words, a larger discrepancy from the true underlying theoretical density. And vice versa, or on the other hand, if you want to lower variability in the case of kde by making age or band with larger, your estimate will be further away from the actual data, at least on average. So in some sense, you are making a larger bias, not allowing your assumptions to be adapted more closely to the data. The data that were observed.\n\nThis phenomenon appears everywhere in data science, statistics, machine learning, and it's an art of finding optimal situation where neither bias nor variance are large. But again, the problem is if you want variance smaller, you will make bias larger and vice versa more and bias various trait of later. And here is an app where I play with the normal distribution. So in this histogram, you see sample from normal distribution of size 50 here of size 200,010 1,000.\n\nAnd let me plot kde for Gaussian kernel or with Gaussian kernel. Now, if I play with randomly generating new plots, new data. You see how kde varies a lot here when sample size is 50. Whereas for n equals 10 thousand, it varies very little.\n\nSo the larger the sample size is, the closer the data and their histogram resembles the actual distribution. The theoretical distribution from which they come, this is intuitively quite reasonable. Simply, more data means you have more information about the distribution. And of course, the more information you have, the better your accuracy in estimation by the histogram and also by the corresponding kde.\n\nNow, what would happen if I make variability of the underlying distribution larger? Then the variability of the data would be larger, and that's obvious, especially for n equals 50. Whereas when sample size is 10000, it's still barely noticeable for this particular sigma. If sigma, the variability of the original data or standard deviation of the original data is smaller than the variability from one sample to another will be smaller.\n\nAnd so the ccdi would better estimate the true density. And if I change my you, that actually doesn't affect the accuracy or variability, it just shifts everything. So variability for normal distribution from data set to data set from sample to sample is only affected by sigma.","potentialAction":{"@type":"SeekToAction","target":"https://canvas.asu.edu/courses/200987/pages/module-2-learning-materials?module_item_id=14590467&wtime={seek_to_second_number}","startOffset-input":"required name=seek_to_second_number"}}</script><script class="w-json-ld" type="application/ld+json" id="w-json-ldwistia_55">{"@context":"http://schema.org/","@id":"https://fast.wistia.net/embed/iframe/tap53x91t1","@type":"VideoObject","duration":"PT12M24S","name":"19-Normal-RV-Part2","thumbnailUrl":"https://embed-ssl.wistia.com/deliveries/23565e535a5e301c02046bd2983a571812f9c623.jpg?image_crop_resized=640x405","embedUrl":"https://fast.wistia.net/embed/iframe/tap53x91t1","uploadDate":"2021-09-10T22:46:35.000Z","description":"a DAT401 - Samara video","contentUrl":"https://embed-ssl.wistia.com/deliveries/55d61e930e3d62942c0f9b92cc202de6c3fc0313.m3u8","transcript":"In the previous video, we started from binomial random variable with the n trials and probability of success was one half. And we rescaled it and centered it, so we normalized it to s.n. star, a new random variable, which is not binomial anymore. And then we were taking larger and larger number of trials, and we saw that the histogram of essence star or actually rescaled histogram was approaching to some shape that is determined by this smooth curve and that this curve here also a probability histogram of essence star was rescaled. So that rather than the height of the bar over some possible value, as star was representing probability, we actually had the area of that bar representing the probability that essence star takes that corresponding value.\n\nAnd so let's now see an example of computing probability that essence star is in between two values, let's say 1.6 negative, 1.6 and 1.2. And here I set and to be so that square root of NPQ is 5.\n\nAnd that means that two consecutive values of essence star differ by 0.2 or one fifth. That's why you see one possible value of essence star is zero. Then a point to then 0.4\n\nand so on. And let me draw the vertical lines x equals a the lower bar of this interval and x equals B the upper bar upper end of the interval a B. OK, so now probability that essence star is in between these two values, and B is simply sum of the areas of these darker shaded rectangles. This is because all possible values of essence start that are in between and B are exactly the middle points of these sides of all these rectangles that you can see here.\n\nAnd so this probability is some of the probabilities that essence start takes this value, then this value, then this value and so on, plus probability that it takes this value. But the probability that essence star takes, let's say this value is exactly the area of this rectangle. That's how we rescale the probability histogram so that we exactly have this feature. That probability that essence star takes particular value is equal to the area of the rectangle over that point.\n\nNow, note that this total sum of these rectangles is approximately equal. I mean, their areas is approximately equal to the area below this curve, this smooth curve and in between these yellow bars or in between values and b, or in this case, negative 1.6 and 1.2. Obviously, there is an error in this approximation because we here have some overshooting and then undershooting overshooting, undershooting.\n\nAnd so on, and also we see here some extra area left from the yellow line. But if we increase number of trials, this approximation is getting better and better because this overshooting and undershooting. And the half of the bar on the edges is getting smaller. And the reason why these bars are getting thinner because each of them is of length exactly equal to the difference between two consecutive values of essence star.\n\nAnd that difference is this delta x or, in other words, 1 over square root of NPQ. So when DNA goes to infinity, 1 over square root of n PCU goes to zero, so the width of each bar goes to zero. And so the approximation is getting better and better. And now we can define random variable z to be a random variable that can take any possible real number.\n\nSo that probability that random variable z falls in between any two values like here, these two probability that the z falls into this interval from a to B is exactly equal to the area below this bell shaped curve, the area below what we call density of z. So this darker maroon area, but in between these two yellow lines. And as n is getting larger and larger probability of essence star, falling in between and B is getting closer and closer to exactly this area below the density and in between and B And in probability and statistics such behavior when probability that essence star falling in between and B is getting closer and closer to probability that z falls in between and B.\n\nIn such situation, we say that essence star converges in distribution to the random variables z. And we also see that we can indeed define the random variable z in this way, using this density and having that probability that z falls in between and B is exactly this darker area and that is true for any two values and B. By the way, this curve here or this density does not stop here and here it spreads out above x-axis in both directions towards negative infinity and positive infinity. It's just that its value is so small, positive but very small that we don't even need to draw and we can anyways draw over the limited range.\n\nBut the point is that for every x, this value is strictly positive. It's just that for extreme values of x, it's very small. Also, another property of the density is that the area from minus infinity to infinity below this density curve is exactly equal to 1. It makes sense because that's the probability that z random variable falls in between negative infinity and infinity.\n\nIn other words, that z is a real number. And this is actually intuitively easy to grasp when we think of how we got this density. Well, that was a curve that the original rescaled histogram of essence star the curve that this original histogram is resembling when n is large. But the area of the probability histogram rescale probability is histogram for essence star is exactly equal to the area that, as in Star falls in between and B.\n\nAnd if we allow and B to B negative infinity and positive infinity probability that essence star falls in between negative infinity and positive infinity is, of course, one on one hand. And on the other hand, that probability is represented by a total area of the histogram. And as we increase RN, the histogram is resembling the density curve. It's getting the shape of the density of z.\n\nAnd so the area of the histogram, which is always constant, converges to the area of this limiting curve. So the limiting curve should have area one below it. Also, this z distribution, we call it standard normal, and it's just one of the family members, from the family of normal distributions. In general, random variable x is called normal.\n\nIf its density is as given here, where mu is some constant arbitrary real number and sigma or sigma squared is also constant. Sigma is positive or sigma squared is positive. It's not difficult to see that the graph of f of x is symmetric around the mu. So when mu is zero, this is what we have when we shift.\n\nLet's say u is negative 4. I should extend the range, let's say, from minus 12 to 12 then. So as we move mu, we shift this graph. So the shape remains the same, but the graph is shifted.\n\nAnd if we change a sigma. The spread changes, so for smaller sigma, the values of x are concentrated more around the peak, which is at mu. And if sigma is larger, the spread is larger. But in any case, for any two values meu, any real number and any positive sigma.\n\nThe area below the curve is always one. And this red curve is for a special case when mu is 0 and sigma is one, which is our z distribution from previous story.","potentialAction":{"@type":"SeekToAction","target":"https://canvas.asu.edu/courses/200987/pages/module-2-learning-materials?module_item_id=14590467&wtime={seek_to_second_number}","startOffset-input":"required name=seek_to_second_number"}}</script><script class="w-json-ld" type="application/ld+json" id="w-json-ldwistia_43">{"@context":"http://schema.org/","@id":"https://fast.wistia.net/embed/iframe/42sn0qb5f6","@type":"VideoObject","duration":"PT13M52S","name":"15-Random-Variables","thumbnailUrl":"https://embed-ssl.wistia.com/deliveries/36f1e0b2644d6c89836f167c43db99d3df79d23c.jpg?image_crop_resized=640x405","embedUrl":"https://fast.wistia.net/embed/iframe/42sn0qb5f6","uploadDate":"2021-09-10T22:46:22.000Z","description":"a DAT401 - Samara video","contentUrl":"https://embed-ssl.wistia.com/deliveries/b6a88e9c140f296df600ef278e7d8fcf6299a27f.m3u8","transcript":"When we collect the data, there is always some randomness involved in the process of collecting the actual values that we got to study this randomness, we introduce random variables. The notion there is one of the most fundamental notions in probability and statistics. Let's see an example. Suppose we play a game in which we count the number of heads in three coin tosses each time we run an experiment, which means toss the coin three times, we get one of the following outcomes.\n\nSo this is our sample space where, for example, HT represents the outcome that in the first toss, we got heads in the second we got tails, followed by tails in the third toss. Since we are interested in number of heads in the three coin tosses, whichever of these outcomes happens, we count the number of heads. And so, for example, if we got outcome HGH, that means the total number of heads is two. And similarly, for all other outcomes, in this way, we get a mapping which for every outcome we assign some value.\n\nOne of the numbers 0 through 3. Because the total number of heads in 3 doses can be any of the integers zero, one, two or three. This mapping, in other words, a function from the sample space to a subset of real numbers is called a random variable. And usually it is denoted by capital letters such as x, y, z, and so on.\n\nSo here is a definition a function x mapping a sample space S to set of real numbers or any subset of it is called a random variable, and the set of all possible values of random variable x is called support set or range of x. In this class, we will denoted by R sub x If we talk about random variable x and its range or its support set. Now, at this point, I do have to give a caveat that in probability theory, actually the definition of random variable is much more subtle than the one that we provided here. Namely, mathematicians define random variable in a more rigorous way, and that way involves introducing so-called sigma field and in particular, Bourhill sigma field defined on sample SpaceX.\n\nAnd then we think of random variable x as Bourhill function. But the good news is that for all practical purposes, we do not need to worry about that. Our definition that we provided here is just fine, so we can think of a random variable as mapping from sample space to a set of real numbers. The only issues with this definition appear only in some very theoretical settings that involve uncountable sample spaces.\n\nThere is sample spaces with all possible outcomes cannot be put into a list into an infinite sequence or infinite list. I just had to mention this caveat because you may find in mathematical literature or in probability, literature some complicated definition of a random variable. So you do not need to worry about that. Now, let's see where random variables appear in statistics and when collecting data.\n\nHere is an example which you may remember from that three. Oh one we have a data set iris in R that comes with the installation of r and calling the command head of iris. We get the list of first six values in a data frame called iris. And this data set consists of iris flowers collected and then measured their Sippel length and width petal length and weed, and recorded which of the three species collected flower belongs to.\n\nAs you can see, first six observations are from the species at tosa, but in this set, there are actually in total three species. And any of these four columns represent the observations from some random variable. So, for example, petal length and let's say within those species there happens to be in this data set 50 observations from the Taza or 50 randomly chosen flowers from cytosol species. And for each of these 50 randomly chosen flowers, petal length was observed or measured, and you see this is given in centimeters.\n\n1.4 centimeters is length of petal of the first flower and 1.3 of the third, and so on. So these values are random because different flowers, even though within sitters have different petal length and we have 50 randomly chosen flowers, so 50 randomly chosen lengths.\n\nAnd also within the other two species, you have two other random variables representing petal length. But for two other species of iris. Now, if the data were collected. So that we did not intentionally look for observations from within species, so maybe species column was also random so that all we were interested in is randomly selecting flowers, iris flowers and in that case, even species is a random variable.\n\nAnyhow, when we collect data, we organize them in these two dimensional array called data frame in r, so there is an object data frame in R R.N. fact, this is just a review from that 301 where you were introduced data frames. I'm just collecting data frames from that 301 discussed in that 3, 2, 1 with random variables that we talk about here. In fact, if you remember, even random variables were introduced in that H3O one, but we are now revisiting this and are defining in a more rigorous way because we need more rigorous mathematical setup. Now we will discuss types of random variables.\n\nWell, that corresponds pretty much to types of data that we can collect. So for example, you see here, this is a numerical variable. And here a species is not numerical. We call it categorical.\n\nLet's have a more thorough discussion, so we divide the data into numerical and categorical, and therefore we have corresponding numerical and categorical random variables of those that are numerical, we can divide them into discrete and continuous. So discrete are those that can take at most comfortably many possible values. In other words, either finitely many or least stably, many or countably many possible values. For example, if we randomly choose a student on campus and ask for number of siblings that the student has, well, that's a random variable that is discrete because the number of siblings can be 0, 1, 2 and so on.\n\nHypothetically, any finite number, at least mathematical model could allow any finite number, although biologically there are some obvious limitations. Another example, number of children that randomly chosen person has or number of defects per day, that certain system experiences and so on. And there are continuous numerical random variables and continuous numerical data. For example, a randomly chosen student, we measure their height, so that's a continuous random variable.\n\nIn practice, we can think of continuous as any random variable that can take unaccountably many possible values. So in the case of height, we can think of any positive real number as hypothetically possible height that a student can have. Similarly, weight. So any time you have the range of a random variable that can be taken to be an interval of real numbers, that is continuous random variable.\n\nIn fact, there is a more precise, more rigorous definition of continuous random variable discussed in that 300. But for all practical purposes, it's just fine the way we use it here and think of continuous random variable as a random variable that can take values from some interval of real numbers or the entire set of real numbers. And apart from numerical, there are also categorical or qualitative random variables and corresponding observations, in other words, data from those random variables. And we can divide categorical random variables into nominal and ordinal nominal, such as marital status, political party or eye color.\n\nSo there are various types of eye color, but there is no natural order in which we can put those values, those I colors. And so when there is no natural order, these random variables are called nominal. Whereas if we have some natural order, such as satisfaction level, so we can put the order so that say very satisfied, then satisfied, then neutral, then unsatisfied and very unsatisfied. So that's a natural order in which we can put possible types of satisfaction level that the customer has customer to whom some service was provided or students grades a, b, C.\n\nSo there is a natural order. So a is better than B. In the following videos, we are going to visualize random variables, and probably that's a review from that three one, but we will build upon that a little bit more.","potentialAction":{"@type":"SeekToAction","target":"https://canvas.asu.edu/courses/200987/pages/module-2-learning-materials?module_item_id=14590467&wtime={seek_to_second_number}","startOffset-input":"required name=seek_to_second_number"}}</script><script class="w-json-ld" type="application/ld+json" id="w-json-ldwistia_49">{"@context":"http://schema.org/","@id":"https://fast.wistia.net/embed/iframe/hqlqt8ijor","@type":"VideoObject","duration":"PT10M17S","name":"17-Binomial-RV","thumbnailUrl":"https://embed-ssl.wistia.com/deliveries/71f9332b08814668ed3d887f8e5bbd6d2051df5b.jpg?image_crop_resized=640x405","embedUrl":"https://fast.wistia.net/embed/iframe/hqlqt8ijor","uploadDate":"2021-09-10T22:46:28.000Z","description":"a DAT401 - Samara video","contentUrl":"https://embed-ssl.wistia.com/deliveries/6f4187549adf2d56c361ca34d08e666ef93f5ccb.m3u8","transcript":"In the previous discussion, we had a random variable x being the number of heads in three coin tosses. We can think of experiment in which we observe the value x to consist of three trials, three coin tosses and in each trial. We are interested in whether heads appeared or didn't appear. If it appears in a particular trial, we can call that success.\n\nIf it doesn't appear. We call that the failure. The random variable x is an example of so-called binomial random variable. So let's see in general what we mean by binomial random variable.\n\nWell, that's a random variable. Or that's a distribution that appears when you repeat the same kind of an experiment multiple times, but fixed the number of times. And in each experiment, you record whether a certain event of interest, let's call it a happened or didn't happen. If it happened, we can call that a success.\n\nIf it didn't. We call that a failure. So in the case of random variable x From previous story, the total number of trials is three, in the event of interest is whether we got heads in a single trial and what is binomial random variable in general. Well, if we repeat the gn trials independently so that the outcome of each trial does not depend on outcomes of other trials, x is number of successes in those and independent trials.\n\nWe denote such random variable in this way. So x, this symbol is called tilde binomial with n being number of independent trials and p is probability of success in a single trial. So in the previous story and is 3 and p probability of success in a single trial is one half assuming that the coin is fair. Also, once we denote by probability of this event of interest, in a single trial, we often also denote by q probability of the complement, in other words, probability of failure.\n\nAnd once again, let us reiterate assumptions, so in each trial, probability of success is p, so it's the same from trial to trial, and we assume that trials are independent in the sense that the outcome of whether we get success or failure in any of the trials will not affect outcomes in any other trial. So the conditional probability knowing that let's say we got the failure in first three trials will not affect the probability of getting success in the fourth trial, it's going to be the same whether we know that there were failures in previous trials or we didn't know. And so x, being a number of heads in 3 coin tosses again is a binomial random variable with n equals 3 number of trials and probability of success being one half.\n\nAnother example is number of 6's in 3 die roles, so we roll a die three times. And again, it's reasonable to assume that outcomes from one trial to another are independent. And so this is binomial because also probability of success in a single trial is one sixth. So here success is getting number 6 and the n is three, just like in three coin tosses.\n\nBut now probability of success is p equals one sixth. In both of these two situations, possible values for x and y are integers zero, one, two or three. In general, if you have n trials, the total number of successes in Rn trials can be any integer from 0 to n And probability that x takes some value k. One of the possible values from 0 to n is, as you can see here, the range for this random variable is the set from 0 through n And probability mass function is given in this way.\n\nTo see where this is coming from. Let's take a look at one possible outcome of the entire experiment that consists of and trials. So this is an outcome such that in the first key trials, we ended up with success. So event A happened followed by n minus k trials that ended up in failure.\n\nIn other words, a compliment happened. If this outcome happened, that means x2 value k because there are exactly k successes to find the probability of such an event. We keep in mind that trials are independent, so getting in the first trial is independent of getting in any other trial. And since we can think of this event as intersection of events, of getting in the first trial.\n\nIn other words, getting success in the first trial and success in the second trial and so on, and success in the case. Trial and failure in Kepler's first trial and failure in K plus second and so on. And failure in the 10th trial. Well, these and events are independent by our assumption of the experiment.\n\nAnd so the probability of their intersection is product of their probabilities. So p of a times, p of a times, blah blah blah. Times p of a times p of a compliment, times a compliment and so on. So that's p to the Keq to the n minus k, but that's not the only outcome for which ex takes value k.\n\nIn fact, any rearrangement of these kA's and n minus k a complements gives rise to x equals k. And so we can think of event that x is equal to k as the union of these events, and each such event or outcome is disjoint or differs from any other. So probability that x is k is some of the probabilities of such outcomes. But each of these outcomes have the same probability, namely p to the Keq to the n minus k because we just rearranged the same number of A's and the same number of a complements.\n\nSo probability that x is equal to k is p to the Keq to the n minus k times. The total number of rearrangements of exactly kA's and n minus k a complements. So it remains to determine in how many ways can we make such rearrangements? Well, note that if you have an placeholders or n places where you are to put exactly ks and n minus k complements well, you first need to choose in which place holders you are going to put a well that is choosing k place holders out of n.\n\nOnce you decide where you are going to put a a, complements must be put in the remaining n minus k place holders. That means the total number of these rearrangements is the same as the number of ways of choosing k out of and place holders to put event a. And that is n choose k. So that's why we have here n choose k.\n\nYou can check that this formula does work. In the example of number of heads in 3 coin tosses, and now you can just apply this formula in the case of random variable y. We then equals 3 and p equals one sixth. And so q is 5 six.\n\nPlease make sure that you understand this formula rather than memorizing it.","potentialAction":{"@type":"SeekToAction","target":"https://canvas.asu.edu/courses/200987/pages/module-2-learning-materials?module_item_id=14590467&wtime={seek_to_second_number}","startOffset-input":"required name=seek_to_second_number"}}</script>
  
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#394B58">
  <meta name="robots" content="noindex,nofollow">
  <meta name="apple-itunes-app" content="app-id=480883488">
<link rel="manifest" href="https://canvas.asu.edu/web-app-manifest/manifest.json">
  <meta name="sentry-trace" content="1fda293c41944849aa440e8a9fa1b376-318a18239fc943e0-0">
  <title>Module 2: Learning Materials: DAT 401: Stat Model Inference Data Sci (2024 Fall A)</title>

  <link rel="preload" href="https://du11hjcvx0uqb.cloudfront.net/dist/fonts/lato/extended/Lato-Regular-bd03a2cc27.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="preload" href="https://du11hjcvx0uqb.cloudfront.net/dist/fonts/lato/extended/Lato-Bold-cccb897485.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="preload" href="https://du11hjcvx0uqb.cloudfront.net/dist/fonts/lato/extended/Lato-Italic-4eb103b4d1.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="./Module 2_ Learning Materials_ DAT 401_ Stat Model Inference Data Sci (2024 Fall A)_files/fonts-6ee09b0b2f.css" media="screen">
  <link rel="stylesheet" href="./Module 2_ Learning Materials_ DAT 401_ Stat Model Inference Data Sci (2024 Fall A)_files/variables-high_contrast-7dd4b80918af0e0218ec0229e4bd5873.css" media="all">
  <link rel="stylesheet" href="./Module 2_ Learning Materials_ DAT 401_ Stat Model Inference Data Sci (2024 Fall A)_files/common-7f4ad40842.css" media="all">
  <link rel="stylesheet" href="./Module 2_ Learning Materials_ DAT 401_ Stat Model Inference Data Sci (2024 Fall A)_files/wiki_page-9ab9e5b496.css" media="screen">
  <link rel="apple-touch-icon" href="https://du11hjcvx0uqb.cloudfront.net/dist/images/apple-touch-icon-585e5d997d.png">
  <link rel="icon" type="image/x-icon" href="https://du11hjcvx0uqb.cloudfront.net/dist/images/favicon-e10d657a73.ico">

  <link rel="stylesheet" href="./Module 2_ Learning Materials_ DAT 401_ Stat Model Inference Data Sci (2024 Fall A)_files/asu-prod.css" media="all">
  
  <script src="./Module 2_ Learning Materials_ DAT 401_ Stat Model Inference Data Sci (2024 Fall A)_files/E-v1.js" async=""></script><script async="" src="./Module 2_ Learning Materials_ DAT 401_ Stat Model Inference Data Sci (2024 Fall A)_files/analytics.js"></script><script type="text/javascript" async="" src="./Module 2_ Learning Materials_ DAT 401_ Stat Model Inference Data Sci (2024 Fall A)_files/js"></script><script async="" src="./Module 2_ Learning Materials_ DAT 401_ Stat Model Inference Data Sci (2024 Fall A)_files/gtm.js"></script><script type="text/javascript" async="" src="./Module 2_ Learning Materials_ DAT 401_ Stat Model Inference Data Sci (2024 Fall A)_files/heap-3001039959.js"></script><script>if (navigator.userAgent.match(/(MSIE|Trident\/)/)) location.replace('/ie-is-not-supported.html')</script>
  <script>
    INST = {"environment":"production","allowMediaComments":true,"kalturaSettings":{"domain":"nv.instructuremedia.com","resource_domain":"nv.instructuremedia.com","rtmp_domain":"iad.rtmp.instructuremedia.com","partner_id":"9","subpartner_id":"0","player_ui_conf":"0","kcw_ui_conf":"0","upload_ui_conf":"0","max_file_size_bytes":534773760,"do_analytics":false,"hide_rte_button":false,"js_uploader":true},"logPageViews":true,"editorButtons":[{"name":"Commons Favorites","id":262,"favorite":false,"url":"https://lor.instructure.com/api/lti/favorite-resources","icon_url":"https://lor.instructure.com/img/icon_commons.png","canvas_icon_class":null,"width":800,"height":400,"use_tray":true,"always_on":false,"description":"\u003cp\u003eFind and share course content\u003c/p\u003e\n"},{"name":"Google Apps","id":264,"favorite":false,"url":"https://google-drive-lti-iad-prod.instructure.com/lti/rce-content-selection","icon_url":"https://google-drive-lti-iad-prod.instructure.com/icon.png","canvas_icon_class":null,"width":700,"height":600,"use_tray":false,"always_on":false,"description":"\u003cp\u003eAllows you to pull in documents from Google Drive to Canvas\u003c/p\u003e\n"},{"name":"PlayPosit ASUO/EdPlus","id":1016,"favorite":false,"url":"https://www.playposit.com/LTI/launch","icon_url":"https://az857794.vo.msecnd.net/images/favicon.ico","canvas_icon_class":null,"width":900,"height":675,"use_tray":false,"always_on":false,"description":"\u003cp\u003ePlayPosit is interactive video engagement.\u003c/p\u003e\n"},{"name":"Google Drive","id":72244,"favorite":false,"url":"https://assignments.google.com/lti/e","icon_url":"https://www.gstatic.com/images/branding/product/1x/drive_16dp.png","canvas_icon_class":null,"width":690,"height":530,"use_tray":false,"always_on":false,"description":"\u003cp\u003eCollect, analyze and grade student work with Google Assignments\u003c/p\u003e\n"},{"name":"Pearson Links","id":171283,"favorite":false,"url":"https://interop.pearson.com/launch","icon_url":"https://www.pearson.com/us/content/dam/one-dot-com/one-dot-com/us/en/images/PearsonThumbprint-RGB-127x138.png/_jcr_content/renditions/cq5dam.web.1600.9600.png","canvas_icon_class":null,"width":800,"height":400,"use_tray":false,"always_on":false,"description":"\u003cp\u003eAccess Pearson\u003c/p\u003e\n"},{"name":"YouTube","id":194887,"favorite":true,"url":"https://www.edu-apps.org/lti_public_resources/?tool_id=youtube","icon_url":"https://www.edu-apps.org/assets/lti_public_resources/youtube_icon.png","canvas_icon_class":null,"width":560,"height":600,"use_tray":false,"always_on":false,"description":"\u003cp\u003eSearch publicly available YouTube videos. A new icon will show up in your course rich editor letting you search YouTube and click to embed videos in your course material.\u003c/p\u003e\n"},{"name":"Library Resource Organizer","id":269079,"favorite":true,"url":"https://arizona-asu.alma.exlibrisgroup.com/lti/v3/launch/01ASU_INST/LMS_CANVAS_2","icon_url":"https://lib.asu.edu/images/quote-left-canvastool.png","canvas_icon_class":null,"width":900,"height":650,"use_tray":false,"always_on":false,"description":"\u003cp\u003eInstructors can easily build lists of resources from inside and outside the ASU Library that include all material types allowing students to access all course materials in one place and from any device.\u003c/p\u003e\n"},{"name":"Insert EquatIO Equation","id":354413,"favorite":false,"url":"https://equatio-lti.texthelp.com","icon_url":"https://cdn-equatio-lti.texthelp.com/icon.png","canvas_icon_class":null,"width":800,"height":520,"use_tray":false,"always_on":false,"description":""},{"name":"Add Wiley Resources","id":366338,"favorite":false,"url":"https://lti.education.wiley.com/wpng/api/v1/ltiadvantage/oidc/login","icon_url":"https://www.wiley.com/college/wileyplus/images/resource_discovery_tool_50x50.png","canvas_icon_class":null,"width":1160,"height":660,"use_tray":false,"always_on":false,"description":"\u003cp\u003eWiley\u003c/p\u003e\n"},{"name":"InScribe Community","id":517529,"favorite":false,"url":"https://inscribe.education/organizations/asu/lti13/launch","icon_url":"https://dvsuxav2ni8t.cloudfront.net/images/inscribe-logo~d04ff4de6d016b3e0055de64a167d25f.min.png","canvas_icon_class":null,"width":700,"height":650,"use_tray":false,"always_on":false,"description":"\u003cp\u003eInScribes virtual community platform gives students access to the information, people, and resources they need to succeed.\u003c/p\u003e\n"}]};
    ENV = {"ASSET_HOST":"https://du11hjcvx0uqb.cloudfront.net","active_brand_config_json_url":"https://du11hjcvx0uqb.cloudfront.net/dist/brandable_css/default/variables-high_contrast-7dd4b80918af0e0218ec0229e4bd5873.json","active_brand_config":null,"confetti_branding_enabled":false,"url_to_what_gets_loaded_inside_the_tinymce_editor_css":["https://du11hjcvx0uqb.cloudfront.net/dist/brandable_css/default/variables-high_contrast-7dd4b80918af0e0218ec0229e4bd5873.css","https://du11hjcvx0uqb.cloudfront.net/dist/brandable_css/new_styles_high_contrast/bundles/what_gets_loaded_inside_the_tinymce_editor-795277483f.css","https://du11hjcvx0uqb.cloudfront.net/dist/brandable_css/no_variables/bundles/fonts-6ee09b0b2f.css"],"url_for_high_contrast_tinymce_editor_css":["https://du11hjcvx0uqb.cloudfront.net/dist/brandable_css/default/variables-high_contrast-7dd4b80918af0e0218ec0229e4bd5873.css","https://du11hjcvx0uqb.cloudfront.net/dist/brandable_css/new_styles_high_contrast/bundles/what_gets_loaded_inside_the_tinymce_editor-795277483f.css","https://du11hjcvx0uqb.cloudfront.net/dist/brandable_css/no_variables/bundles/fonts-6ee09b0b2f.css"],"current_user_id":"776848","current_user_global_id":"72360000000776848","current_user_heap_id":"uu-2-90770e20c2ddcbc54e74f3f00870ff22887d8ac60b57a4cfe1f2ffde44621b84-nHHP1jS4lnwGPxZv9rfVToTna6MdWBcw5Y1hlepv","current_user_roles":["user","student"],"current_user_is_student":true,"current_user_types":[],"current_user_disabled_inbox":false,"current_user_visited_tabs":null,"discussions_reporting":true,"files_domain":"cluster201.canvas-user-content.com","group_information":null,"DOMAIN_ROOT_ACCOUNT_ID":"72360000000000001","DOMAIN_ROOT_ACCOUNT_UUID":"nHHP1jS4lnwGPxZv9rfVToTna6MdWBcw5Y1hlepv","k12":false,"help_link_name":"Help","help_link_icon":"help","use_high_contrast":true,"auto_show_cc":false,"disable_celebrations":false,"disable_keyboard_shortcuts":false,"LTI_LAUNCH_FRAME_ALLOWANCES":["geolocation *","microphone *","camera *","midi *","encrypted-media *","autoplay *","clipboard-write *","display-capture *"],"DEEP_LINKING_POST_MESSAGE_ORIGIN":"https://canvas.asu.edu","comment_library_suggestions_enabled":false,"SETTINGS":{"open_registration":false,"collapse_global_nav":false,"release_notes_badge_disabled":false,"can_add_pronouns":true,"show_sections_in_course_tray":true},"RAILS_ENVIRONMENT":"Production","IN_PACED_COURSE":false,"SENTRY_FRONTEND":{"dsn":"https://355a1d96717e4038ac25aa852fa79a8f@relay-iad.sentry.insops.net/388","org_slug":"instructure","base_url":"https://sentry.insops.net","normalized_route":"/courses/{course_id}/pages/{id}","errors_sample_rate":"0.005","traces_sample_rate":"0.005","url_deny_pattern":"instructure-uploads.*amazonaws.com","revision":"canvas-lms@20240828.240"},"DATA_COLLECTION_ENDPOINT":"https://canvas-frontend-data-iad-prod.inscloudgate.net/submit","DOMAIN_ROOT_ACCOUNT_SFID":"001A00000086eLx","DIRECT_SHARE_ENABLED":false,"CAN_VIEW_CONTENT_SHARES":false,"FEATURES":{"featured_help_links":true,"account_level_blackout_dates":false,"render_both_to_do_lists":false,"commons_new_quizzes":false,"course_paces_redesign":true,"course_paces_for_students":true,"explicit_latex_typesetting":false,"media_links_use_attachment_id":true,"permanent_page_links":true,"selective_release_backend":true,"selective_release_ui_api":true,"selective_release_edit_page":true,"enhanced_course_creation_account_fetching":false,"instui_for_import_page":false,"multiselect_gradebook_filters":true,"assignment_edit_placement_not_on_announcements":false,"platform_service_speedgrader":false,"instui_header":false,"rce_find_replace":true,"courses_popout_sisid":true,"dashboard_graphql_integration":false,"discussion_checkpoints":false,"speedgrader_studio_media_capture":true,"product_tours":false,"usage_rights_discussion_topics":true,"granular_permissions_manage_users":true,"create_course_subaccount_picker":true,"file_verifiers_for_quiz_links":true,"lti_deep_linking_module_index_menu_modal":true,"lti_dynamic_registration":true,"lti_registrations_next":false,"lti_multiple_assignment_deep_linking":true,"lti_overwrite_user_url_input_select_content_dialog":true,"buttons_and_icons_root_account":true,"extended_submission_state":false,"scheduled_page_publication":true,"send_usage_metrics":true,"rce_transform_loaded_content":false,"mobile_offline_mode":false,"react_discussions_post":true,"instui_nav":false,"enhanced_developer_keys_tables":true,"lti_registrations_discover_page":false,"account_level_mastery_scales":false,"non_scoring_rubrics":true,"top_navigation_placement":false,"rubric_criterion_range":true,"lti_migration_info":true,"lti_toggle_placements":true,"embedded_release_notes":true,"canvas_k6_theme":false,"new_math_equation_handling":true},"current_user":{"id":"776848","anonymous_id":"gnf4","display_name":"Charles Coonce","avatar_image_url":"https://canvas.asu.edu/images/messages/avatar-50.png","html_url":"https://canvas.asu.edu/about/776848","pronouns":null,"avatar_is_fallback":true,"email":"ccoonce@asu.edu"},"current_user_is_admin":false,"page_view_update_url":"/page_views/732d917d-5b47-4c7b-ae23-292602997772?page_view_token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpIjoiNzMyZDkxN2QtNWI0Ny00YzdiLWFlMjMtMjkyNjAyOTk3NzcyIiwidSI6NzIzNjAwMDAwMDA3NzY4NDgsImMiOiIyMDI0LTA5LTA2VDE3OjE5OjMzLjc4WiJ9.YI7UqOHuxSQan7qyXhOSgvfvb1Jz8iUlsC0BYSOasrA","context_asset_string":"course_200987","ping_url":"https://canvas.asu.edu/api/v1/courses/200987/ping","TIMEZONE":"America/Denver","CONTEXT_TIMEZONE":"America/Phoenix","LOCALES":["en"],"BIGEASY_LOCALE":"en_US","FULLCALENDAR_LOCALE":"en","MOMENT_LOCALE":"en","rce_auto_save_max_age_ms":86400000,"K5_USER":false,"USE_CLASSIC_FONT":false,"K5_HOMEROOM_COURSE":false,"K5_SUBJECT_COURSE":false,"LOCALE_TRANSLATION_FILE":"/dist/javascripts/translations/en-67c25a6858.json","ACCOUNT_ID":"11","user_cache_key":"clloZ0l1VEtYVk9JMkt6MVk4ZGFzb2hvUUV4WTE1cXFoWm9PeThwenZ5Zlc9\nO1twLTA/OntQXz1IVXBncmFxZTtuamFsa2hwdm9pdWxraW1tYXFld2c=\n","current_context":{"id":"200987","name":"DAT 401: Stat Model Inference Data Sci (2024 Fall A)","type":"Course","url":"https://canvas.asu.edu/courses/200987"},"WIKI_RIGHTS":{"read":true},"PAGE_RIGHTS":{"read":true},"DEFAULT_EDITING_ROLES":"teachers","WIKI_PAGES_PATH":"/courses/200987/pages","WIKI_PAGE":{"title":"Module 2: Learning Materials","created_at":"2024-08-21T00:08:34-06:00","url":"module-2-learning-materials","editing_roles":"teachers","page_id":"8453204","published":true,"hide_from_students":false,"front_page":false,"html_url":"https://canvas.asu.edu/courses/200987/pages/module-2-learning-materials","todo_date":null,"publish_at":null,"updated_at":"2024-08-21T00:08:34-06:00","locked_for_user":false,"body":"\u003ch3\u003eVideos\u0026nbsp;\u003c/h3\u003e\n\u003ch4\u003e2hr 33min\u003c/h4\u003e\n\u003cp\u003e\u003ca href=\"#v1\"\u003eM2.1. Independent Events (11:05)\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"#v2\"\u003eM2.2. Independence of Multiple Events (14:54)\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"#v3\"\u003eM2.3. Random Variables (13:52)\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"#v4\"\u003eM2.4. Prob. Mass Function (PMF); Prob. Histogram (7:06)\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"#v5\"\u003eM2.5. Binomial RV (10:17)\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"#v6\"\u003eM2.6. Normal RV - Part 1 (10:39)\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"#v7\"\u003eM2.7. Normal RV - Part 2 (12:24)\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"#v8\"\u003eM2.8. Continuous RVs (17:06)\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"#v9\"\u003eM2.9. Histograms (9:43)\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"#v10\"\u003eM2.10. Kernel Density Estimate (KDE) (16:49)\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"#v11\"\u003eM2.11. KDE - Example (11:25)\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"#v12\"\u003eM2.12. Population Mean and Sample Mean (17:13)\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ch3\u003eReadings\u003c/h3\u003e\n\u003cp\u003eLECTURE SLIDES\u003c/p\u003e\n\u003cp\u003e\u003ca id=\"42107931\" class=\"instructure_file_link instructure_scribd_file inline_disabled\" href=\"https://canvas.asu.edu/courses/200987/files/90830926?wrap=1\" target=\"_blank\" data-canvas-previewable=\"true\" data-api-endpoint=\"https://canvas.asu.edu/api/v1/courses/200987/files/90830926\" data-api-returntype=\"File\"\u003eCh2-M2.1-M2.2\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca id=\"42117691\" class=\"instructure_file_link instructure_scribd_file inline_disabled\" href=\"https://canvas.asu.edu/courses/200987/files/90830932?wrap=1\" target=\"_blank\" data-canvas-previewable=\"true\" data-api-endpoint=\"https://canvas.asu.edu/api/v1/courses/200987/files/90830932\" data-api-returntype=\"File\"\u003eCh3-M2.3-M2.11\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca id=\"42116133\" class=\"instructure_file_link instructure_scribd_file inline_disabled\" href=\"https://canvas.asu.edu/courses/200987/files/90830928?wrap=1\" target=\"_blank\" data-canvas-previewable=\"true\" data-api-endpoint=\"https://canvas.asu.edu/api/v1/courses/200987/files/90830928\" data-api-returntype=\"File\"\u003eCh4-M2.12\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eApart from slides, you can also use online lecture notes. Typically, online lecture notes are a bit more elaborate version of the content in the lecture slides, have interactive plots and are easier to navigate.\u003c/p\u003e\n\u003cp\u003eLECTURE NOTES:\u003c/p\u003e\n\u003cp\u003e\u003ca class=\"inline_disabled\" href=\"https://math.la.asu.edu/~samara/StatMod-lectures/conditional-probability.html\" target=\"_blank\"\u003eCh2. Conditional Probability\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca class=\"inline_disabled\" href=\"https://math.la.asu.edu/~samara/StatMod-lectures/random-variables-and-data.html\" target=\"_blank\"\u003eCh3. Random Variables and Data\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca class=\"inline_disabled\" href=\"https://math.la.asu.edu/~samara/StatMod-lectures/measures-of-location-and-spread.html\" target=\"_blank\"\u003eCh4. Measures of Location and Spread\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003cp\u003e\u003ca id=\"v1\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ch2\u003eM2.1. Independent Events (11:05)\u003c/h2\u003e\n\u003cp\u003e\u003ciframe class=\"wistia_embed\" title=\"13-Independent-Events Video\" src=\"https://fast.wistia.net/embed/iframe/s9fr1h3qgi\" width=\"640\" height=\"405\" name=\"wistia_embed\" allow=\"autoplay; fullscreen\"\u003e\u003c/iframe\u003e\u003c/p\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003cp\u003e\u003ca id=\"v2\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ch2\u003eM2.2. Independence of Multiple Events (14:54)\u003c/h2\u003e\n\u003cp\u003e\u003ciframe class=\"wistia_embed\" title=\"14-Independence-of-Multiple-Events Video\" src=\"https://fast.wistia.net/embed/iframe/6nwiabgta4\" width=\"640\" height=\"405\" name=\"wistia_embed\" allow=\"autoplay; fullscreen\"\u003e\u003c/iframe\u003e\u003c/p\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003cp\u003e\u003ca id=\"v3\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ch2\u003eM2.3. Random Variables (13:52)\u003c/h2\u003e\n\u003cp\u003e\u003ciframe class=\"wistia_embed\" title=\"15-Random-Variables Video\" src=\"https://fast.wistia.net/embed/iframe/42sn0qb5f6\" width=\"640\" height=\"405\" name=\"wistia_embed\" allow=\"autoplay; fullscreen\"\u003e\u003c/iframe\u003e\u003c/p\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003cp\u003e\u003ca id=\"v4\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ch2\u003eM2.4. Prob. Mass Function (PMF); Prob. Histogram (7:06)\u003c/h2\u003e\n\u003cp\u003e\u003ciframe class=\"wistia_embed\" title=\"16-PMF-Prob-Histogram Video\" src=\"https://fast.wistia.net/embed/iframe/kwrq5519mf\" width=\"640\" height=\"405\" name=\"wistia_embed\" allow=\"autoplay; fullscreen\"\u003e\u003c/iframe\u003e\u003c/p\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003cp\u003e\u003ca id=\"v5\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ch2\u003eM2.5. Binomial RV (10:17)\u003c/h2\u003e\n\u003cp\u003e\u003ciframe class=\"wistia_embed\" title=\"17-Binomial-RV Video\" src=\"https://fast.wistia.net/embed/iframe/hqlqt8ijor\" width=\"640\" height=\"405\" name=\"wistia_embed\" allow=\"autoplay; fullscreen\"\u003e\u003c/iframe\u003e\u003c/p\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003cp\u003e\u003ca id=\"v6\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ch2\u003eM2.6. Normal RV - Part 1 (10:39)\u003c/h2\u003e\n\u003cp\u003e\u003ciframe class=\"wistia_embed\" title=\"18-Normal-RV-Part1 Video\" src=\"https://fast.wistia.net/embed/iframe/dqpmduckbv\" width=\"640\" height=\"405\" name=\"wistia_embed\" allow=\"autoplay; fullscreen\"\u003e\u003c/iframe\u003e\u003c/p\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003cp\u003e\u003ca id=\"v7\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ch2\u003eM2.7. Normal RV - Part 2 (12:24)\u003c/h2\u003e\n\u003cp\u003e\u003ciframe class=\"wistia_embed\" title=\"19-Normal-RV-Part2 Video\" src=\"https://fast.wistia.net/embed/iframe/tap53x91t1\" width=\"640\" height=\"405\" name=\"wistia_embed\" allow=\"autoplay; fullscreen\"\u003e\u003c/iframe\u003e\u003c/p\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003cp\u003e\u003ca id=\"v8\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ch2\u003eM2.8. Continuous RVs (17:06)\u003c/h2\u003e\n\u003cp\u003e\u003ciframe class=\"wistia_embed\" title=\"20-Continuous-RVs Video\" src=\"https://fast.wistia.net/embed/iframe/wfeuor0ci5\" width=\"640\" height=\"405\" name=\"wistia_embed\" allow=\"autoplay; fullscreen\"\u003e\u003c/iframe\u003e\u003c/p\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003cp\u003e\u003ca id=\"v9\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ch2\u003eM2.9. Histograms (9:43)\u003c/h2\u003e\n\u003cp\u003e\u003ciframe class=\"wistia_embed\" title=\"21-Histograms Video\" src=\"https://fast.wistia.net/embed/iframe/sc24zos85v\" width=\"640\" height=\"405\" name=\"wistia_embed\" allow=\"autoplay; fullscreen\"\u003e\u003c/iframe\u003e\u003c/p\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003cp\u003e\u003ca id=\"v10\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ch2\u003eM2.10. Kernel Density Estimate (KDE) (16:49)\u003c/h2\u003e\n\u003cp\u003e\u003ciframe class=\"wistia_embed\" title=\"22-KDE Video\" src=\"https://fast.wistia.net/embed/iframe/xbvpah4u8n\" width=\"640\" height=\"405\" name=\"wistia_embed\" allow=\"autoplay; fullscreen\"\u003e\u003c/iframe\u003e\u003c/p\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003cp\u003e\u003ca id=\"v11\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ch2\u003eM2.11. KDE - Example (11:25)\u003c/h2\u003e\n\u003cp\u003e\u003ciframe class=\"wistia_embed\" title=\"23-KDE-Example Video\" src=\"https://fast.wistia.net/embed/iframe/6ak31neeya\" width=\"640\" height=\"405\" name=\"wistia_embed\" allow=\"autoplay; fullscreen\"\u003e\u003c/iframe\u003e\u003c/p\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003cp\u003e\u003ca id=\"v12\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ch2\u003eM2.12. Population Mean and Sample Mean (17:13)\u003c/h2\u003e\n\u003cp\u003e\u003ciframe class=\"wistia_embed\" title=\"24-Population-Mean-and-Sample-Mean Video\" src=\"https://fast.wistia.net/embed/iframe/s3kk8ib5gv\" width=\"640\" height=\"405\" name=\"wistia_embed\" allow=\"autoplay; fullscreen\"\u003e\u003c/iframe\u003e\u003c/p\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e"},"WIKI_PAGE_REVISION":"1","WIKI_PAGE_SHOW_PATH":"/courses/200987/pages/module-2-learning-materials","WIKI_PAGE_EDIT_PATH":"/courses/200987/pages/module-2-learning-materials/edit","WIKI_PAGE_HISTORY_PATH":"/courses/200987/pages/module-2-learning-materials/revisions","COURSE_ID":"200987","MODULES_PATH":"/courses/200987/modules","wiki_page_menu_tools":[],"wiki_index_menu_tools":[],"DISPLAY_SHOW_ALL_LINK":false,"CAN_SET_TODO_DATE":false,"TITLE_AVAILABILITY_PATH":"/api/v1/courses/200987/page_title_availability","badge_counts":{"submissions":3},"notices":[],"active_context_tab":"pages"};
    BRANDABLE_CSS_HANDLEBARS_INDEX = [["new_styles_normal_contrast","new_styles_high_contrast","new_styles_normal_contrast_rtl","new_styles_high_contrast_rtl"],{"10":["908ffbc673",0,"d5c9044c6e",2],"15":["c8540c43a4",0,"0a2196be1d",2],"19":["df5777ed9c"],"61":["1fb36890db","b83a603352","4bcf82f85a","0b171e092c"],"67":["700335fb7b",0,"1cc2485e2c",2],"71":["8ac0336ef0","bf3093677a","a8146a011b","449171073f"],"06":["ba28819778",0,"96b99aafe5",2],"f0":["b349f31f5e",0,0,0],"c8":["718c8509f5","33cd4c40e3","03d0fbcbe8","ec23096f0d"],"1e":["6eb4ecac8e","4100cb65ce","0faf4716c8","bed54fd75e"],"b3":["e5da23fb43","0911fc8ed3","05b2bb5a6f","ba2585de5c"],"0c":["4dae5befd2",0,"c18876be89",2],"da":["b5a7f9cd8f","a4e5066985","773390ae11","8e9071910c"],"1d":["2128789890",0,"e568085637",2],"08":["64bff5a97d"],"e2":["11429f119a"],"9f":["d39b291ba6",0,0,0],"2b":["c491abf31e","b11dc54da6","b58622671f","6bb4a7ae9e"],"2c":["8a926fc28b",0,0,0],"c2":["6f2721ae01"],"9c":["c31821c764",0,"1693aba1da",2],"c5":["44c6024769","31150a4a27","53dd277fa9","5fe61c91c2"],"f2":["51574f9b13"]}]
    REMOTES = {};
  </script>
  <script src="./Module 2_ Learning Materials_ DAT 401_ Stat Model Inference Data Sci (2024 Fall A)_files/variables-high_contrast-7dd4b80918af0e0218ec0229e4bd5873.js" defer="defer"></script>
  <script src="./Module 2_ Learning Materials_ DAT 401_ Stat Model Inference Data Sci (2024 Fall A)_files/Denver-8b016888d3.js" defer="defer"></script>
  <script src="./Module 2_ Learning Materials_ DAT 401_ Stat Model Inference Data Sci (2024 Fall A)_files/Phoenix-9c778f3ab6.js" defer="defer"></script>
  <script src="./Module 2_ Learning Materials_ DAT 401_ Stat Model Inference Data Sci (2024 Fall A)_files/en_US-80a0ce259b.js" defer="defer"></script>
  <script src="./Module 2_ Learning Materials_ DAT 401_ Stat Model Inference Data Sci (2024 Fall A)_files/react-entry-d6972cd7700667c0.js" crossorigin="anonymous" defer="defer"></script>
  <script src="./Module 2_ Learning Materials_ DAT 401_ Stat Model Inference Data Sci (2024 Fall A)_files/main-entry-f00d809ecd786b56.js" crossorigin="anonymous" defer="defer"></script>
<script>
//<![CDATA[
(window.bundles || (window.bundles = [])).push('wiki_page_show');
(window.bundles || (window.bundles = [])).push('navigation_header');
//]]>
</script>
  
  <script type="text/javascript">
    window.heap=window.heap||[],heap.load=function(e,t){window.heap.appid=e,window.heap.config=t=t||{};var r=document.createElement("script");r.type="text/javascript",r.async=!0,r.src="https://cdn.heapanalytics.com/js/heap-"+e+".js";var a=document.getElementsByTagName("script")[0];a.parentNode.insertBefore(r,a);for(var n=function(e){return function(){heap.push([e].concat(Array.prototype.slice.call(arguments,0)))}},p=["addEventProperties","addUserProperties","clearEventProperties","identify","resetIdentity","removeEventProperty","setEventProperties","track","unsetEventProperty"],o=0;o<p.length;o++)heap[p[o]]=n(p[o])};
    heap.load("3001039959");
    heap.clearEventProperties();
    heap.addEventProperties({'Base.appName': 'Canvas'});
    setTimeout(() => {
      if (ENV.current_user_heap_id && ENV.current_user_heap_id !== heap.identity) {
        heap.identify(ENV.current_user_heap_id);
      }
      props = {}
      if (ENV.current_user_roles) {
        props['role'] = ENV.current_user_roles[ENV.current_user_roles.length - 1]
      }
      if (ENV.DOMAIN_ROOT_ACCOUNT_UUID) {
        props['Canvas.accountId'] = ENV.DOMAIN_ROOT_ACCOUNT_UUID
      }
      if (ENV.DOMAIN_ROOT_ACCOUNT_SFID) {
        props['Canvas.salesforceAccountId'] = ENV.DOMAIN_ROOT_ACCOUNT_SFID
      }
      if (Object.keys(props).length > 0) {
        heap.addUserProperties(props);
      }
    }, 1000);
  </script>
<style type="text/css"></style><link data-webpack="canvas-lms:chunk-58416" rel="stylesheet" href="./Module 2_ Learning Materials_ DAT 401_ Stat Model Inference Data Sci (2024 Fall A)_files/58416-chunk-62380000ac7f0b98.css"><style id="inert-style">
[inert] {
  pointer-events: none;
  cursor: default;
}

[inert], [inert] * {
  user-select: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
}
</style><link rel="stylesheet" type="text/css" href="./Module 2_ Learning Materials_ DAT 401_ Stat Model Inference Data Sci (2024 Fall A)_files/canvas.efa431314734a7b57eb41c1afa7285a1.css"></head>

<body class="with-left-side course-menu-expanded padless-content pages primary-nav-expanded context-course_200987 responsive_student_grades_page show webkit chrome no-touch ally-tooltips-enabled">

<noscript>
  <div role="alert" class="ic-flash-static ic-flash-error">
    <div class="ic-flash__icon" aria-hidden="true">
      <i class="icon-warning"></i>
    </div>
    <h1>You need to have JavaScript enabled in order to access this site.</h1>
  </div>
</noscript>




<div id="flash_message_holder"></div>
<div id="flash_screenreader_holder" role="alert" aria-live="assertive" aria-relevant="additions" class="screenreader-only" aria-atomic="true"></div>

<div id="application" class="ic-app">
  




<header id="mobile-header" class="no-print">
  <button type="button" class="Button Button--icon-action-rev Button--large mobile-header-hamburger">
    <i class="icon-solid icon-hamburger"></i>
    <span id="mobileHeaderInboxUnreadBadge" class="menu-item__badge" style="min-width: 0; top: 12px; height: 12px; right: 6px; display:none;"></span>
    <span class="screenreader-only">Dashboard</span>
  </button><div style="position: absolute; width: 100%;"><button id="mobile-custom-asu-header-links" class="Button Button--icon-action-rev Button--large  mobile-header-hamburger"><i class="icon-solid icon-hamburger"></i><span class="screenreader-only">ASU Header Links</span></button></div>
  <div class="mobile-header-space"></div>
    <a class="mobile-header-title expandable" href="https://canvas.asu.edu/courses/200987" role="button" aria-controls="mobileContextNavContainer">
      <div>2024FallA-X-DAT401-76302</div>
        <div>Module 2: Learning Materials</div>
    </a>
    <div class="mobile-header-space"></div>
    <button type="button" class="Button Button--icon-action-rev Button--large mobile-header-arrow" aria-label="Navigation Menu">
      <i class="icon-arrow-open-down" id="mobileHeaderArrowIcon"></i>
    </button>
</header>
<nav id="mobileContextNavContainer" aria-expanded="false"></nav>

<header id="header" class="ic-app-header no-print ">
  <a href="https://canvas.asu.edu/courses/200987/pages/module-2-learning-materials?module_item_id=14590467#content" id="skip_navigation_link">Skip To Content</a>
  <div role="region" class="ic-app-header__main-navigation" aria-label="Global Navigation">
    <ul id="menu" class="ic-app-header__menu-list">
        <li class="menu-item ic-app-header__menu-list-item ">
          <a id="global_nav_profile_link" role="button" href="https://canvas.asu.edu/profile/settings" class="ic-app-header__menu-list-link">
            <div class="menu-item-icon-container">
              <div aria-hidden="true" class="fs-exclude ic-avatar ">
                <img src="./Module 2_ Learning Materials_ DAT 401_ Stat Model Inference Data Sci (2024 Fall A)_files/avatar-50.png" alt="Charles Coonce">
              </div>
              <span class="menu-item__badge"></span>
            </div>
            <div class="menu-item__text">
              Account
            </div>
          </a>
        </li>
      <li class="ic-app-header__menu-list-item ">
        <a id="global_nav_dashboard_link" href="https://canvas.asu.edu/" class="ic-app-header__menu-list-link">
          <div class="menu-item-icon-container" aria-hidden="true">
            <svg xmlns="http://www.w3.org/2000/svg" class="ic-icon-svg ic-icon-svg--dashboard" version="1.1" x="0" y="0" viewBox="0 0 280 200" enable-background="new 0 0 280 200" xml:space="preserve"><path d="M273.09,180.75H197.47V164.47h62.62A122.16,122.16,0,1,0,17.85,142a124,124,0,0,0,2,22.51H90.18v16.29H6.89l-1.5-6.22A138.51,138.51,0,0,1,1.57,142C1.57,65.64,63.67,3.53,140,3.53S278.43,65.64,278.43,142a137.67,137.67,0,0,1-3.84,32.57ZM66.49,87.63,50.24,71.38,61.75,59.86,78,76.12Zm147,0L202,76.12l16.25-16.25,11.51,11.51ZM131.85,53.82v-23h16.29v23Zm15.63,142.3a31.71,31.71,0,0,1-28-16.81c-6.4-12.08-15.73-72.29-17.54-84.25a8.15,8.15,0,0,1,13.58-7.2c8.88,8.21,53.48,49.72,59.88,61.81a31.61,31.61,0,0,1-27.9,46.45ZM121.81,116.2c4.17,24.56,9.23,50.21,12,55.49A15.35,15.35,0,1,0,161,157.3C158.18,152,139.79,133.44,121.81,116.2Z"></path></svg>

          </div>
          <div class="menu-item__text">
            Dashboard
          </div>
        </a>
      </li>
        <li class="menu-item ic-app-header__menu-list-item ic-app-header__menu-list-item--active" aria-current="page">
          <a id="global_nav_courses_link" role="button" href="https://canvas.asu.edu/courses" class="ic-app-header__menu-list-link">
            <div class="menu-item-icon-container" aria-hidden="true">
              <svg xmlns="http://www.w3.org/2000/svg" class="ic-icon-svg ic-icon-svg--courses" version="1.1" x="0" y="0" viewBox="0 0 280 259" enable-background="new 0 0 280 259" xml:space="preserve"><path d="M73.31,198c-11.93,0-22.22,8-24,18.73a26.67,26.67,0,0,0-.3,3.63v.3a22,22,0,0,0,5.44,14.65,22.47,22.47,0,0,0,17.22,8H200V228.19h-134V213.08H200V198Zm21-105.74h90.64V62H94.3ZM79.19,107.34V46.92H200v60.42Zm7.55,30.21V122.45H192.49v15.11ZM71.65,16.71A22.72,22.72,0,0,0,49,39.36V190.88a41.12,41.12,0,0,1,24.32-8h157V16.71ZM33.88,39.36A37.78,37.78,0,0,1,71.65,1.6H245.36V198H215.15v45.32h22.66V258.4H71.65a37.85,37.85,0,0,1-37.76-37.76Z"></path></svg>

            </div>
            <div class="menu-item__text">
              Courses
            </div>
          </a>
        </li>
        <li class="menu-item ic-app-header__menu-list-item ">
          <a id="global_nav_groups_link" role="button" href="https://canvas.asu.edu/groups" class="ic-app-header__menu-list-link">
            <div class="menu-item-icon-container" aria-hidden="true">
              <svg xmlns="http://www.w3.org/2000/svg" class="ic-icon-svg ic-icon-svg--groups" viewBox="0 0 200 135"><path d="M134.5 129.4c0-1.1 0-19.8-6.2-31.1-4.5-8.5-16.4-12.4-35-19.2-1.7-.6-3.4-1.1-5.1-1.7v-8.5c5.6-5.1 8.5-12.4 8.5-20.3V29.4C96.6 13 83.6 0 67.2 0S37.9 13 37.9 29.4v19.2c0 7.3 3.4 14.7 8.5 20.3v8.5c-1.7.6-3.4 1.1-5.1 1.7-18.6 6.2-30.5 10.7-35 19.2C0 109.6 0 128.8 0 129.4c0 3.4 2.3 5.6 5.6 5.6h123.7c3.5 0 5.7-2.3 5.2-5.6zm-123.2-5.7c.6-5.6 1.7-14.7 3.4-19.8C17 98.8 30 94.3 43.5 89.8c2.8-1.1 5.6-2.3 9-3.4 2.3-.6 4-2.8 4-5.1V66.7c0-1.7-.6-3.4-1.7-4.5-4-3.4-6.2-8.5-6.2-13.6V29.4c0-10.2 7.9-18.1 18.1-18.1s18.1 7.9 18.1 18.1v19.2c0 5.1-2.3 10.2-6.2 13.6-1.1 1.1-1.7 2.8-1.7 4.5v14.7c0 2.3 1.7 4.5 4 5.1 2.8 1.1 6.2 2.3 9 3.4 13.6 5.1 26.6 9.6 28.8 14.1 2.8 5.1 4 13.6 4.5 19.8H11.3zM196 79.1c-2.8-6.2-11.3-9.6-22.6-13.6l-1.7-.6v-3.4c4.5-4 6.8-9.6 6.8-15.8V35c0-12.4-9.6-22-22-22s-22 10.2-22 22v10.7c0 6.2 2.3 11.9 6.8 15.8V65l-1.7.6c-7.3 2.8-13 4.5-16.9 7.3-1.7 1.1-2.3 2.8-2.3 5.1.6 1.7 1.7 3.4 3.4 4.5 7.9 4 12.4 7.3 14.1 10.7 2.3 4.5 4 10.2 5.1 18.1.6 2.3 2.8 4.5 5.6 4.5h45.8c3.4 0 5.6-2.8 5.6-5.1 0-3.9 0-24.3-4-31.6zm-42.9 25.4c-1.1-6.8-2.8-12.4-5.1-16.9-1.7-4-5.1-6.8-9.6-10.2 1.7-1.1 3.4-1.7 5.1-2.3l5.6-2.3c1.7-.6 3.4-2.8 3.4-5.1v-9.6c0-1.7-.6-3.4-2.3-4.5-2.8-1.7-4.5-5.1-4.5-8.5V34.5c0-6.2 4.5-10.7 10.7-10.7s10.7 5.1 10.7 10.7v10.7c0 3.4-1.7 6.2-4.5 8.5-1.1 1.1-2.3 2.8-2.3 4.5v10.2c0 2.3 1.1 4.5 3.4 5.1l5.6 2.3c6.8 2.3 15.3 5.6 16.4 7.9 1.7 2.8 2.8 12.4 2.8 20.9h-35.4z"></path></svg>

            </div>
            <div class="menu-item__text">
              Groups
            </div>
          </a>
        </li>
      <li class="menu-item ic-app-header__menu-list-item ">
        <a id="global_nav_calendar_link" href="https://canvas.asu.edu/calendar" class="ic-app-header__menu-list-link">
          <div class="menu-item-icon-container" aria-hidden="true">
            <svg xmlns="http://www.w3.org/2000/svg" class="ic-icon-svg ic-icon-svg--calendar" version="1.1" x="0" y="0" viewBox="0 0 280 280" enable-background="new 0 0 280 280" xml:space="preserve"><path d="M197.07,213.38h16.31V197.07H197.07Zm-16.31,16.31V180.76h48.92v48.92Zm-48.92-16.31h16.31V197.07H131.85Zm-16.31,16.31V180.76h48.92v48.92ZM66.62,213.38H82.93V197.07H66.62ZM50.32,229.68V180.76H99.24v48.92Zm146.75-81.53h16.31V131.85H197.07Zm-16.31,16.31V115.54h48.92v48.92Zm-48.92-16.31h16.31V131.85H131.85Zm-16.31,16.31V115.54h48.92v48.92ZM66.62,148.15H82.93V131.85H66.62ZM50.32,164.46V115.54H99.24v48.92ZM34,262.29H246V82.93H34ZM246,66.62V42.16A8.17,8.17,0,0,0,237.84,34H213.38v8.15a8.15,8.15,0,1,1-16.31,0V34H82.93v8.15a8.15,8.15,0,0,1-16.31,0V34H42.16A8.17,8.17,0,0,0,34,42.16V66.62Zm-8.15-48.92a24.49,24.49,0,0,1,24.46,24.46V278.6H17.71V42.16A24.49,24.49,0,0,1,42.16,17.71H66.62V9.55a8.15,8.15,0,0,1,16.31,0v8.15H197.07V9.55a8.15,8.15,0,1,1,16.31,0v8.15Z"></path></svg>

          </div>
          <div class="menu-item__text">
            Calendar
          </div>
        </a>
      </li>
      <li class="menu-item ic-app-header__menu-list-item ">
      <!-- TODO: Add back global search when available -->
        <a id="global_nav_conversations_link" href="https://canvas.asu.edu/conversations" class="ic-app-header__menu-list-link">
          <div class="menu-item-icon-container">
            <span aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" class="ic-icon-svg ic-icon-svg--inbox" version="1.1" x="0" y="0" viewBox="0 0 280 280" enable-background="new 0 0 280 280" xml:space="preserve"><path d="M91.72,120.75h96.56V104.65H91.72Zm0,48.28h80.47V152.94H91.72Zm0-96.56h80.47V56.37H91.72Zm160.94,34.88H228.52V10.78h-177v96.56H27.34A24.17,24.17,0,0,0,3.2,131.48V244.14a24.17,24.17,0,0,0,24.14,24.14H252.66a24.17,24.17,0,0,0,24.14-24.14V131.48A24.17,24.17,0,0,0,252.66,107.34Zm0,16.09a8.06,8.06,0,0,1,8,8v51.77l-32.19,19.31V123.44ZM67.58,203.91v-177H212.42v177ZM27.34,123.44H51.48v79.13L19.29,183.26V131.48A8.06,8.06,0,0,1,27.34,123.44ZM252.66,252.19H27.34a8.06,8.06,0,0,1-8-8V202l30,18H230.75l30-18v42.12A8.06,8.06,0,0,1,252.66,252.19Z"></path></svg>
</span>
            <span class="menu-item__badge"></span>
          </div>
          <div class="menu-item__text">
            Inbox
          </div>
        </a>
      </li>
        <li class="menu-item ic-app-header__menu-list-item">
          <a id="global_nav_history_link" role="button" href="https://canvas.asu.edu/courses/200987/pages/module-2-learning-materials?module_item_id=14590467#" class="ic-app-header__menu-list-link">
            <div class="menu-item-icon-container" aria-hidden="true">
              <svg viewBox="0 0 1920 1920" class="ic-icon-svg menu-item__icon svg-icon-history" version="1.1" xmlns="http://www.w3.org/2000/svg">
    <path d="M960 112.941c-467.125 0-847.059 379.934-847.059 847.059 0 467.125 379.934 847.059 847.059 847.059 467.125 0 847.059-379.934 847.059-847.059 0-467.125-379.934-847.059-847.059-847.059M960 1920C430.645 1920 0 1489.355 0 960S430.645 0 960 0s960 430.645 960 960-430.645 960-960 960m417.905-575.955L903.552 988.28V395.34h112.941v536.47l429.177 321.77-67.765 90.465z" stroke="none" stroke-width="1" fill-rule="evenodd"></path>
</svg>
            </div>
            <div class="menu-item__text">
              History
            </div>
          </a>
        </li>
        

      <li class="ic-app-header__menu-list-item">
        <a id="global_nav_help_link" role="button" class="ic-app-header__menu-list-link" data-track-category="help system" data-track-label="help button" href="https://help.instructure.com/">
          <div class="menu-item-icon-container" role="presentation">
              <svg xmlns="http://www.w3.org/2000/svg" class="ic-icon-svg menu-item__icon svg-icon-help" version="1.1" x="0" y="0" viewBox="0 0 200 200" enable-background="new 0 0 200 200" xml:space="preserve" fill="currentColor"><path d="M100,127.88A11.15,11.15,0,1,0,111.16,139,11.16,11.16,0,0,0,100,127.88Zm8.82-88.08a33.19,33.19,0,0,1,23.5,23.5,33.54,33.54,0,0,1-24,41.23,3.4,3.4,0,0,0-2.74,3.15v9.06H94.42v-9.06a14.57,14.57,0,0,1,11.13-14,22.43,22.43,0,0,0,13.66-10.27,22.73,22.73,0,0,0,2.31-17.37A21.92,21.92,0,0,0,106,50.59a22.67,22.67,0,0,0-19.68,3.88,22.18,22.18,0,0,0-8.65,17.64H66.54a33.25,33.25,0,0,1,13-26.47A33.72,33.72,0,0,1,108.82,39.8ZM100,5.2A94.8,94.8,0,1,0,194.8,100,94.91,94.91,0,0,0,100,5.2m0,178.45A83.65,83.65,0,1,1,183.65,100,83.73,83.73,0,0,1,100,183.65" transform="translate(-5.2 -5.2)"></path></svg>

            <span class="menu-item__badge"></span>
          </div>
          <div class="menu-item__text">
            Help
          </div>
</a>      </li>
    <li id="global_nav_accessibility_menu" class="ic-app-header__menu-list-item"><a id="global_nav_accessibility_link" href="https://accessibility.asu.edu/canvas-student-accessibility" class="ic-app-header__menu-list-link" target="_blank"><div class="menu-item-icon-container" role="presentation"><span class="svg-accessibility-holder"><svg version="1.1" class="ic-icon-svg" id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 26 26" style="enable-background:new 0 0 26 26;" xml:space="preserve"><g><path class="st0" d="M20.5,8.5c-4.5,1.7-9.8,1.7-15,0L4.6,8.3v3.2L5,11.7l0.1,0.1c1.7,0.5,3.2,0.9,4.6,1.1v9.7h3.4v-5h0.5v5h3.3 v-9.8c1.3-0.2,2.6-0.5,4.1-1.1l0.4-0.2V8.3L20.5,8.5z M15.5,11.6v9.7h-0.7v-5h-3.1v5H11v-9.7l-0.6,0c-1.3-0.2-2.7-0.5-4.4-1V10 c2.5,0.7,4.9,1.1,7.2,1.1c2.3,0,4.6-0.4,6.9-1.1v0.8c-1.2,0.4-2.5,0.7-4,0.9H15.5z"></path><path class="st0" d="M13.2,9.1c1.6,0,2.8-1.3,2.8-2.9s-1.3-2.9-2.8-2.9c-1.6,0-2.9,1.3-2.9,2.9S11.6,9.1,13.2,9.1z M13.2,7.9 c-0.9,0-1.6-0.6-1.6-1.6c0-0.9,0.7-1.5,1.6-1.5c0.8,0,1.5,0.6,1.5,1.5C14.7,7.2,14.1,7.9,13.2,7.9z"></path><path class="st0" d="M13,0.6C6.2,0.6,0.6,6.2,0.6,13S6.2,25.3,13,25.3S25.3,19.8,25.3,13S19.8,0.6,13,0.6z M13,24.1 c-6.1,0-11-5-11-11.1s5-11,11-11s11.1,5,11.1,11S19.1,24.1,13,24.1z"></path></g></svg></span></div><div class="menu-item__text">Accessibility</div></a></li></ul>
  </div>
  <div class="ic-app-header__secondary-navigation">
    <ul class="ic-app-header__menu-list">
      <li class="menu-item ic-app-header__menu-list-item">
        <a id="primaryNavToggle" role="button" href="https://canvas.asu.edu/courses/200987/pages/module-2-learning-materials?module_item_id=14590467#" class="ic-app-header__menu-list-link ic-app-header__menu-list-link--nav-toggle" aria-label="Minimize global navigation" title="Minimize global navigation">
          <div class="menu-item-icon-container" aria-hidden="true">
            <svg xmlns="http://www.w3.org/2000/svg" class="ic-icon-svg ic-icon-svg--navtoggle" version="1.1" x="0" y="0" width="40" height="32" viewBox="0 0 40 32" xml:space="preserve">
  <path d="M39.5,30.28V2.48H37.18v27.8Zm-4.93-13.9L22.17,4,20.53,5.61l9.61,9.61H.5v2.31H30.14l-9.61,9.61,1.64,1.64Z"></path>
</svg>

          </div>
        </a>
      </li>
    </ul>
  </div>
  <div id="global_nav_tray_container"></div>
  <div id="global_nav_tour"></div>
</header>


  <div id="instructure_ajax_error_box">
    <div style="text-align: right; background-color: #fff;"><a href="https://canvas.asu.edu/courses/200987/pages/module-2-learning-materials?module_item_id=14590467#" class="close_instructure_ajax_error_box_link">Close</a></div>
    <iframe id="instructure_ajax_error_result" src="./Module 2_ Learning Materials_ DAT 401_ Stat Model Inference Data Sci (2024 Fall A)_files/saved_resource.html" style="border: 0;" title="Error"></iframe>
  </div>

  <div id="wrapper" class="ic-Layout-wrapper"><div class="asuHeaderPadding"></div><div id="custom-asu-header-links"><ul id="custom-asu-header-ul" class="closed"><li><a class="Button" href="http://www.asu.edu/">ASU Home</a></li><li><a class="Button" href="https://my.asu.edu/">My ASU</a></li><li><a class="Button" href="http://www.asu.edu/colleges/">Colleges &amp; Schools</a></li><li><a class="Button" href="http://www.asu.edu/map/">Map &amp; Locations</a></li><li><a class="Button" href="http://www.asu.edu/contactasu/">Contact Us</a></li></ul></div>
        <div class="ic-app-nav-toggle-and-crumbs no-print">
            <button type="button" id="courseMenuToggle" class="Button Button--link ic-app-course-nav-toggle" aria-live="polite" aria-label="Hide Courses Navigation Menu">
              <i class="icon-hamburger" aria-hidden="true"></i>
            </button>

          <div class="ic-app-crumbs ">
              <nav id="breadcrumbs" role="navigation" aria-label="breadcrumbs"><ul><li class="home"><a href="https://canvas.asu.edu/"><span class="ellipsible ellipsis" style="max-width: 200px;"><i class="icon-home" title="My Dashboard">
  <span class="screenreader-only">My Dashboard</span>
</i>
</span></a></li><li><a href="https://canvas.asu.edu/courses/200987"><span class="ellipsible ellipsis" style="max-width: 200px;">2024FallA-X-DAT401-76302</span></a></li><li><a href="https://canvas.asu.edu/courses/200987/pages"><span class="ellipsible ellipsis" style="max-width: 200px;">Pages</span></a></li><li><span class="ellipsible ellipsis" style="max-width: 200px;">Module 2: Learning Materials</span></li></ul></nav>
          </div>


          <div class="right-of-crumbs">
          </div>

        </div>
    <div id="main" class="ic-Layout-columns">
        <div class="ic-Layout-watermark"></div>
        <div id="left-side" class="ic-app-course-menu ic-sticky-on list-view" style="display: block">
          <div id="sticky-container" class="ic-sticky-frame has-scrollbar">
              <span id="section-tabs-header-subtitle" class="ellipsis">2024 Fall A</span>
            <nav role="navigation" aria-label="Courses Navigation Menu"><ul id="section-tabs"><li class="section"><a href="https://canvas.asu.edu/courses/200987" class="home" tabindex="0">Home</a></li><li class="section"><a href="https://canvas.asu.edu/courses/200987/announcements" class="announcements" tabindex="0">Announcements</a></li><li class="section"><a href="https://canvas.asu.edu/courses/200987/assignments/syllabus" class="syllabus" tabindex="0">Syllabus</a></li><li class="section"><a href="https://canvas.asu.edu/courses/200987/modules" class="modules" tabindex="0">Modules</a></li><li class="section"><a href="https://canvas.asu.edu/courses/200987/discussion_topics" class="discussions" tabindex="0">Discussions</a></li><li class="section"><a href="https://canvas.asu.edu/courses/200987/grades" class="grades" tabindex="0">Grades<b class="nav-badge">3</b></a></li><li class="section"><a href="https://canvas.asu.edu/courses/200987/external_tools/542175" class="context_external_tool_542175" tabindex="0">ASU Course Policies</a></li><li class="section"><a href="https://canvas.asu.edu/courses/200987/external_tools/542177" class="context_external_tool_542177" tabindex="0">Resources</a></li><li class="section"><a href="https://canvas.asu.edu/courses/200987/external_tools/542176" class="context_external_tool_542176" tabindex="0">Accessibility</a></li><li class="section"><a href="https://canvas.asu.edu/courses/200987/external_tools/542178" class="context_external_tool_542178" tabindex="0">Time in AZ</a></li><li class="section"><a href="https://canvas.asu.edu/courses/200987/external_tools/24" class="context_external_tool_24" tabindex="0">Chat</a></li><li class="section"><a href="https://canvas.asu.edu/courses/200987/external_tools/199537" class="context_external_tool_199537" tabindex="0">Honorlock ASUO</a></li><li class="section"><a href="https://canvas.asu.edu/courses/200987/quizzes" class="quizzes" tabindex="0">Quizzes</a></li><li class="section"><a href="https://canvas.asu.edu/courses/200987/external_tools/450178?display=borderless" class="context_external_tool_450178" target="_blank" tabindex="0">ASU Tutoring</a></li></ul></nav>
          </div>
        </div>
      <div id="not_right_side" class="ic-app-main-content">
        <div id="content-wrapper" class="ic-Layout-contentWrapper">
          
          <div id="content" class="ic-Layout-contentMain" role="main">
            

  

<div id="wiki_page_show">

<div class="show-content user_content clearfix enhanced" data-resource-type="wiki_page.body" data-resource-id="8453204" data-lti-page-content="true">
  
    <h1 class="page-title">Module 2: Learning Materials<button aria-haspopup="dialog" class="ally-accessible-versions ally-add-tooltip" data-id="page:8453204" data-ally-content-id="page:8453204" data-ally-richcontent-eid="page:8453204" aria-label="Alternative formats" title="Alternative formats">
    <span class="ally-prominent-af-download-button"></span>
</button></h1>
  
  
  
    <h3>Videos&nbsp;</h3>
<h4>2hr 33min</h4>
<p><a href="https://canvas.asu.edu/courses/200987/pages/module-2-learning-materials?module_item_id=14590467#v1">M2.1. Independent Events (11:05)</a></p>
<p><a href="https://canvas.asu.edu/courses/200987/pages/module-2-learning-materials?module_item_id=14590467#v2">M2.2. Independence of Multiple Events (14:54)</a></p>
<p><a href="https://canvas.asu.edu/courses/200987/pages/module-2-learning-materials?module_item_id=14590467#v3">M2.3. Random Variables (13:52)</a></p>
<p><a href="https://canvas.asu.edu/courses/200987/pages/module-2-learning-materials?module_item_id=14590467#v4">M2.4. Prob. Mass Function (PMF); Prob. Histogram (7:06)</a></p>
<p><a href="https://canvas.asu.edu/courses/200987/pages/module-2-learning-materials?module_item_id=14590467#v5">M2.5. Binomial RV (10:17)</a></p>
<p><a href="https://canvas.asu.edu/courses/200987/pages/module-2-learning-materials?module_item_id=14590467#v6">M2.6. Normal RV - Part 1 (10:39)</a></p>
<p><a href="https://canvas.asu.edu/courses/200987/pages/module-2-learning-materials?module_item_id=14590467#v7">M2.7. Normal RV - Part 2 (12:24)</a></p>
<p><a href="https://canvas.asu.edu/courses/200987/pages/module-2-learning-materials?module_item_id=14590467#v8">M2.8. Continuous RVs (17:06)</a></p>
<p><a href="https://canvas.asu.edu/courses/200987/pages/module-2-learning-materials?module_item_id=14590467#v9">M2.9. Histograms (9:43)</a></p>
<p><a href="https://canvas.asu.edu/courses/200987/pages/module-2-learning-materials?module_item_id=14590467#v10">M2.10. Kernel Density Estimate (KDE) (16:49)</a></p>
<p><a href="https://canvas.asu.edu/courses/200987/pages/module-2-learning-materials?module_item_id=14590467#v11">M2.11. KDE - Example (11:25)</a></p>
<p><a href="https://canvas.asu.edu/courses/200987/pages/module-2-learning-materials?module_item_id=14590467#v12">M2.12. Population Mean and Sample Mean (17:13)</a></p>
<p>&nbsp;</p>
<h3>Readings</h3>
<p>LECTURE SLIDES</p>
<p><span class="instructure_file_holder link_holder instructure_file_link_holder ally-file-link-holder"><a id="42107931" class="inline_disabled preview_in_overlay" href="https://canvas.asu.edu/courses/200987/files/90830926?wrap=1" target="_blank" data-canvas-previewable="true" data-api-endpoint="https://canvas.asu.edu/api/v1/courses/200987/files/90830926" data-api-returntype="File" data-id="90830926">Ch2-M2.1-M2.2</a><div class="inline-block ally-enhancement ally-user-content-dropdown">
    <a href="https://canvas.asu.edu/courses/200987/pages/module-2-learning-materials?module_item_id=14590467#" role="button" tabindex="0" class="al-trigger">
        <img style="width:16px; height:16px" src="./Module 2_ Learning Materials_ DAT 401_ Stat Model Inference Data Sci (2024 Fall A)_files/svg_icon_download.svg" alt="" role="presentation">
        <span class="screenreader-only">Actions</span>
    </a>
    <ul class="al-options">
        <li><a href="https://canvas.asu.edu/courses/200987/files/90830926?wrap=1" class="inline_disabled preview_in_overlay" data-id="90830926">Preview</a></li>
        <li><a href="https://canvas.asu.edu/courses/200987/files/90830926/download?download_frd=1" data-id="90830926">Download</a></li>
        <li><a href="https://canvas.asu.edu/courses/200987/pages/module-2-learning-materials?module_item_id=14590467#" class="ally-accessible-versions" data-id="90830926">Alternative formats</a></li>
    </ul>
</div></span></p>
<p><span class="instructure_file_holder link_holder instructure_file_link_holder ally-file-link-holder"><a id="42117691" class="inline_disabled preview_in_overlay" href="https://canvas.asu.edu/courses/200987/files/90830932?wrap=1" target="_blank" data-canvas-previewable="true" data-api-endpoint="https://canvas.asu.edu/api/v1/courses/200987/files/90830932" data-api-returntype="File" data-id="90830932">Ch3-M2.3-M2.11</a><div class="inline-block ally-enhancement ally-user-content-dropdown">
    <a href="https://canvas.asu.edu/courses/200987/pages/module-2-learning-materials?module_item_id=14590467#" role="button" tabindex="0" class="al-trigger">
        <img style="width:16px; height:16px" src="./Module 2_ Learning Materials_ DAT 401_ Stat Model Inference Data Sci (2024 Fall A)_files/svg_icon_download.svg" alt="" role="presentation">
        <span class="screenreader-only">Actions</span>
    </a>
    <ul class="al-options">
        <li><a href="https://canvas.asu.edu/courses/200987/files/90830932?wrap=1" class="inline_disabled preview_in_overlay" data-id="90830932">Preview</a></li>
        <li><a href="https://canvas.asu.edu/courses/200987/files/90830932/download?download_frd=1" data-id="90830932">Download</a></li>
        <li><a href="https://canvas.asu.edu/courses/200987/pages/module-2-learning-materials?module_item_id=14590467#" class="ally-accessible-versions" data-id="90830932">Alternative formats</a></li>
    </ul>
</div></span></p>
<p><span class="instructure_file_holder link_holder instructure_file_link_holder ally-file-link-holder"><a id="42116133" class="inline_disabled preview_in_overlay" href="https://canvas.asu.edu/courses/200987/files/90830928?wrap=1" target="_blank" data-canvas-previewable="true" data-api-endpoint="https://canvas.asu.edu/api/v1/courses/200987/files/90830928" data-api-returntype="File" data-id="90830928">Ch4-M2.12</a><div class="inline-block ally-enhancement ally-user-content-dropdown">
    <a href="https://canvas.asu.edu/courses/200987/pages/module-2-learning-materials?module_item_id=14590467#" role="button" tabindex="0" class="al-trigger">
        <img style="width:16px; height:16px" src="./Module 2_ Learning Materials_ DAT 401_ Stat Model Inference Data Sci (2024 Fall A)_files/svg_icon_download.svg" alt="" role="presentation">
        <span class="screenreader-only">Actions</span>
    </a>
    <ul class="al-options">
        <li><a href="https://canvas.asu.edu/courses/200987/files/90830928?wrap=1" class="inline_disabled preview_in_overlay" data-id="90830928">Preview</a></li>
        <li><a href="https://canvas.asu.edu/courses/200987/files/90830928/download?download_frd=1" data-id="90830928">Download</a></li>
        <li><a href="https://canvas.asu.edu/courses/200987/pages/module-2-learning-materials?module_item_id=14590467#" class="ally-accessible-versions" data-id="90830928">Alternative formats</a></li>
    </ul>
</div></span></p>
<p>Apart from slides, you can also use online lecture notes. Typically, online lecture notes are a bit more elaborate version of the content in the lecture slides, have interactive plots and are easier to navigate.</p>
<p>LECTURE NOTES:</p>
<p><a class="inline_disabled" href="https://math.la.asu.edu/~samara/StatMod-lectures/conditional-probability.html" target="_blank">Ch2. Conditional Probability</a></p>
<p><a class="inline_disabled" href="https://math.la.asu.edu/~samara/StatMod-lectures/random-variables-and-data.html" target="_blank">Ch3. Random Variables and Data</a></p>
<p><a class="inline_disabled" href="https://math.la.asu.edu/~samara/StatMod-lectures/measures-of-location-and-spread.html" target="_blank">Ch4. Measures of Location and Spread</a></p>
<p>&nbsp;</p>
<p><a id="v1"></a></p>
<p>&nbsp;</p>
<h2>M2.1. Independent Events (11:05)</h2>
<p><iframe class="wistia_embed" title="13-Independent-Events Video" src="./Module 2_ Learning Materials_ DAT 401_ Stat Model Inference Data Sci (2024 Fall A)_files/s9fr1h3qgi.html" width="640" height="405" name="wistia_embed" allow="autoplay; fullscreen"></iframe></p>
<p>&nbsp;</p>
<p><a id="v2"></a></p>
<p>&nbsp;</p>
<h2>M2.2. Independence of Multiple Events (14:54)</h2>
<p><iframe class="wistia_embed" title="14-Independence-of-Multiple-Events Video" src="./Module 2_ Learning Materials_ DAT 401_ Stat Model Inference Data Sci (2024 Fall A)_files/6nwiabgta4.html" width="640" height="405" name="wistia_embed" allow="autoplay; fullscreen"></iframe></p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p><a id="v3"></a></p>
<p>&nbsp;</p>
<h2>M2.3. Random Variables (13:52)</h2>
<p><iframe class="wistia_embed" title="15-Random-Variables Video" src="./Module 2_ Learning Materials_ DAT 401_ Stat Model Inference Data Sci (2024 Fall A)_files/42sn0qb5f6.html" width="640" height="405" name="wistia_embed" allow="autoplay; fullscreen"></iframe></p>
<p>&nbsp;</p>
<p><a id="v4"></a></p>
<p>&nbsp;</p>
<h2>M2.4. Prob. Mass Function (PMF); Prob. Histogram (7:06)</h2>
<p><iframe class="wistia_embed" title="16-PMF-Prob-Histogram Video" src="./Module 2_ Learning Materials_ DAT 401_ Stat Model Inference Data Sci (2024 Fall A)_files/kwrq5519mf.html" width="640" height="405" name="wistia_embed" allow="autoplay; fullscreen"></iframe></p>
<p>&nbsp;</p>
<p><a id="v5"></a></p>
<p>&nbsp;</p>
<h2>M2.5. Binomial RV (10:17)</h2>
<p><iframe class="wistia_embed" title="17-Binomial-RV Video" src="./Module 2_ Learning Materials_ DAT 401_ Stat Model Inference Data Sci (2024 Fall A)_files/hqlqt8ijor.html" width="640" height="405" name="wistia_embed" allow="autoplay; fullscreen"></iframe></p>
<p>&nbsp;</p>
<p><a id="v6"></a></p>
<p>&nbsp;</p>
<h2>M2.6. Normal RV - Part 1 (10:39)</h2>
<p><iframe class="wistia_embed" title="18-Normal-RV-Part1 Video" src="./Module 2_ Learning Materials_ DAT 401_ Stat Model Inference Data Sci (2024 Fall A)_files/dqpmduckbv.html" width="640" height="405" name="wistia_embed" allow="autoplay; fullscreen"></iframe></p>
<p>&nbsp;</p>
<p><a id="v7"></a></p>
<p>&nbsp;</p>
<h2>M2.7. Normal RV - Part 2 (12:24)</h2>
<p><iframe class="wistia_embed" title="19-Normal-RV-Part2 Video" src="./Module 2_ Learning Materials_ DAT 401_ Stat Model Inference Data Sci (2024 Fall A)_files/tap53x91t1.html" width="640" height="405" name="wistia_embed" allow="autoplay; fullscreen"></iframe></p>
<p>&nbsp;</p>
<p><a id="v8"></a></p>
<p>&nbsp;</p>
<h2>M2.8. Continuous RVs (17:06)</h2>
<p><iframe class="wistia_embed" title="20-Continuous-RVs Video" src="./Module 2_ Learning Materials_ DAT 401_ Stat Model Inference Data Sci (2024 Fall A)_files/wfeuor0ci5.html" width="640" height="405" name="wistia_embed" allow="autoplay; fullscreen"></iframe></p>
<p>&nbsp;</p>
<p><a id="v9"></a></p>
<p>&nbsp;</p>
<h2>M2.9. Histograms (9:43)</h2>
<p><iframe class="wistia_embed" title="21-Histograms Video" src="./Module 2_ Learning Materials_ DAT 401_ Stat Model Inference Data Sci (2024 Fall A)_files/sc24zos85v.html" width="640" height="405" name="wistia_embed" allow="autoplay; fullscreen"></iframe></p>
<p>&nbsp;</p>
<p><a id="v10"></a></p>
<p>&nbsp;</p>
<h2>M2.10. Kernel Density Estimate (KDE) (16:49)</h2>
<p><iframe class="wistia_embed" title="22-KDE Video" src="./Module 2_ Learning Materials_ DAT 401_ Stat Model Inference Data Sci (2024 Fall A)_files/xbvpah4u8n.html" width="640" height="405" name="wistia_embed" allow="autoplay; fullscreen"></iframe></p>
<p>&nbsp;</p>
<p><a id="v11"></a></p>
<p>&nbsp;</p>
<h2>M2.11. KDE - Example (11:25)</h2>
<p><iframe class="wistia_embed" title="23-KDE-Example Video" src="./Module 2_ Learning Materials_ DAT 401_ Stat Model Inference Data Sci (2024 Fall A)_files/6ak31neeya.html" width="640" height="405" name="wistia_embed" allow="autoplay; fullscreen"></iframe></p>
<p>&nbsp;</p>
<p><a id="v12"></a></p>
<p>&nbsp;</p>
<h2>M2.12. Population Mean and Sample Mean (17:13)</h2>
<p><iframe class="wistia_embed" title="24-Population-Mean-and-Sample-Mean Video" src="./Module 2_ Learning Materials_ DAT 401_ Stat Model Inference Data Sci (2024 Fall A)_files/s3kk8ib5gv.html" width="640" height="405" name="wistia_embed" allow="autoplay; fullscreen"></iframe></p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
  
<div id="assign-to-mount-point"></div>
</div>
</div>
<div id="module_navigation_target"><div style=""><div class="module-sequence-padding"></div>
<div class="module-sequence-footer" role="navigation" aria-label="Module Navigation">
  <div class="module-sequence-footer-content">
    
      <span class="module-sequence-footer-button--previous" data-tooltip="right" data-html-tooltip-title="&lt;i class=&#39;icon-document&#39;&gt;&lt;/i&gt; Module 2: Overview ">
          <a href="https://canvas.asu.edu/courses/200987/modules/items/14590466" class="Button" aria-describedby="msf0-previous-desc" aria-label="Previous Module Item">
            <i class="icon-mini-arrow-left"></i>Previous
            <span id="msf0-previous-desc" class="hidden" hidden="">Previous: Module 2: Overview </span>
          </a>
      </span>
    

    
      <span class="module-sequence-footer-button--next" data-tooltip="left" data-html-tooltip-title="&lt;i class=&#39;icon-assignment&#39;&gt;&lt;/i&gt; Module 2:  HW2">
        <a href="https://canvas.asu.edu/courses/200987/modules/items/14590468" class="Button" aria-describedby="msf0-next-desc" aria-label="Next Module Item">
          Next<i class="icon-mini-arrow-right"></i>
          <span id="msf0-next-desc" class="hidden" hidden="">Next: Module 2:  HW2</span>
        </a>
      </span>
    
  </div>
</div>
</div></div>

          </div>
        </div>
        <div id="right-side-wrapper" class="ic-app-main-content__secondary">
          <aside id="right-side" role="complementary">
            
          </aside>
        </div>
      </div>
    </div>
  </div>



    <div style="display:none;"><!-- Everything inside of this should always stay hidden -->
        <div id="page_view_id">732d917d-5b47-4c7b-ae23-292602997772</div>
    </div>
  <div id="aria_alerts" class="hide-text affix" role="alert" aria-live="assertive"></div>
  <div id="StudentTray__Container"></div>
  <div id="react-router-portals"></div>
  

  <iframe src="./Module 2_ Learning Materials_ DAT 401_ Stat Model Inference Data Sci (2024 Fall A)_files/post_message_forwarding.html" name="post_message_forwarding" title="post_message_forwarding" id="post_message_forwarding" sandbox="allow-scripts allow-same-origin" style="display:none;"></iframe>


  <script>
    Object.assign(
      ENV,
      {}
    )
  </script>

<script>
//<![CDATA[
(window.bundles || (window.bundles = [])).push('inst_fs_service_worker');
//]]>
</script>
  <script src="./Module 2_ Learning Materials_ DAT 401_ Stat Model Inference Data Sci (2024 Fall A)_files/canvas-prod.js" defer="defer"></script>

</div> <!-- #application -->


<div class="tinymce-a11y-checker-container"></div><div id="nav-tray-portal" style="position: relative; z-index: 99;"></div><div role="log" aria-live="assertive" aria-relevant="additions" class="ally-helper-hidden-accessible"></div><div id="hl-aria-live-message-container" aria-live="polite" class="visually-hidden"></div><div id="hl-aria-live-alert-container" role="alert" aria-live="assertive" class="visually-hidden"></div><script type="text/javascript" id="gtm-wistia-tracking">(function(f,c,g){function p(a){var e=g._track.percentages,b={Play:"play",Pause:"pause","Watch to End":"end"},d={};k(["Play","Watch to End"],function(d){g.events[d]&&a.bind(b[d],function(){n(d,a)})});g.events.Pause&&a.bind("pause",function(){1!==a.percentWatched()&&n("Pause",a)});e&&a.bind("secondchange",function(b){b=a.percentWatched();for(var c in e)b>=e[c]&&!d[c]&&(d[c]=!0,n(c,a))})}function q(a){a=r({},{events:{Play:!0,Pause:!0,"Watch to End":!0},percentages:{each:[],every:[]}},a);k(["each","every"],
function(d){var b=a.percentages[d];var c=b;c=Array.isArray_?Array.isArray_(c):"[object Array]"===Object.prototype.toString.call(c);c||(b=[b]);b&&(a.percentages[d]=t(b,Number))});var c=[].concat(a.percentages.each);a.percentages.every&&k(a.percentages.every,function(a){var b=100/a,d=[],e;for(e=1;e<b;e++)d.push(a*e);c=c.concat(u(d,function(a){return 0<a&&100>a}))});var b=v(c,function(a,b){a[b+"%"]=b/100;return a},{});a._track={percentages:b};return a}function w(a){a=a||{};var e=a.name||"dataLayer",
b=a.name||c.GoogleAnalyticsObject||"ga",d="_gaq",h={gtm:function(a,b){f.push({event:"wistiaTrack",attributes:{videoAction:a,videoName:b.name()}})},cl:function(a,b){c[d].push(["_trackEvent","Videos",a,b.name()])},ua:function(a,d){c[b]("send","event","Videos",a,d.name())}};switch(a.type){case "gtm":var f=c[e]=c[e]||[];break;case "ua":c[b]=c[b]||function(){(c[b].q=c[b].q||[]).push(arguments)};c[b].l=+new Date;break;case "cl":c[d]=c[d]||[];break;default:l(c[e])?b&&!l(c[b])?a.type="ua":l(c[d])||l(c[d].push)||
(a.type="cl"):(a.type="gtm",f=c[e]=c[e]||[])}return h[a.type]}function r(){var a=[].slice.call(arguments),c=a.shift(),b,d;for(d=0;d<a.length;d++){var h=a[d];for(b in h)c[b]=h[b]}return c}function k(a,e){if(Array.prototype.forEach_)return a.forEach.call(a,e);var b;for(b=0;b<a.length;b++)e.call(c,a[b],b,a)}function t(a,e){if(Array.prototype.map_)return a.map.call(a,e);var b=[];k(a,function(a,h,f){b.push(e.call(c,a,h,f))});return b}function u(a,e){if(Array.prototype.filter)return a.filter.call(a,e);
var b=[];k(a,function(a,f,g){e.call(c,a,f,g)&&b.push(a)});return b}function v(a,e,b){if(Array.prototype.reduce)return a.reduce.call(a,e,b);var d;for(d=0;d<a.length;d++){var f=a[d];b=e.call(c,b,f,a,d)}return b}function l(a){return"undefined"===typeof a}g=q(g);var m=c._wq=c._wq||[],n=w(g.syntax);m.push({id:"_all",onReady:p});l(c.Wistia)&&(m=f.getElementsByTagName("script")[0],f=f.createElement("script"),f.src="//fast.wistia.net/assets/external/E-v1.js",f.async=!0,m.parentNode.insertBefore(f,m))})(document,
window,{events:{Play:!0,Pause:!0,"Watch to End":!0},percentages:{every:25,each:[10,90]}});</script><style id="wistia_22_style" type="text/css" class="wistia_injected_style">
@font-face {
font-family: 'WistiaPlayerInterNumbersSemiBold';
font-feature-settings: 'tnum' 1;
src: url(data:application/x-font-woff;charset=utf-8;base64,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);
}
</style></body></html>